{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BlackLake Documentation","text":"<p>Welcome to the BlackLake documentation. BlackLake is a production-ready, enterprise-grade data artifact management platform that combines modern technology with comprehensive features for data management, search, governance, and compliance.</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"#one-command-setup","title":"One-Command Setup","text":"<pre><code># Build everything and start the full stack\njust setup-all\n</code></pre>"},{"location":"#alternative-setup-methods","title":"Alternative Setup Methods","text":"<pre><code># Using the quick setup script\n./quick-setup.sh\n\n# Using optimized build script\n./build-optimized.sh dev\n\n# Using Docker directly\ndocker buildx bake local\ndocker compose --profile dev up -d --wait\n</code></pre>"},{"location":"#project-status","title":"\ud83d\udccb Project Status","text":"<p>BlackLake is PRODUCTION-READY with comprehensive implementation of Weeks 1-8:</p>"},{"location":"#completed-features","title":"\u2705 Completed Features","text":"<ul> <li>Week 1: Core Infrastructure (Rust API, PostgreSQL, S3 storage)</li> <li>Week 2: Search &amp; Metadata (JSONB search, Dublin Core, RDF)</li> <li>Week 3: Security &amp; Multi-Tenancy (OIDC auth, RBAC, rate limiting)</li> <li>Week 4: Governance &amp; Safety Rails (branch protection, quotas, webhooks)</li> <li>Week 5: Operational Hardening (multi-arch builds, monitoring, K6 testing)</li> <li>Week 6: Advanced Search &amp; Sessions (Solr integration, server sessions)</li> <li>Week 7: Enterprise Hardening (ABAC policies, data classification, SDKs)</li> <li>Week 8: Federation &amp; AI Features (connectors, semantic search, mobile UX)</li> </ul>"},{"location":"#carryover-items-week-9","title":"\ud83d\udd04 Carryover Items (Week 9)","text":"<ul> <li>Reliability &amp; Disaster Recovery</li> <li>Cost &amp; Lifecycle Governance</li> <li>Access Reviews &amp; Data Egress Controls</li> <li>Performance Baseline &amp; Load Testing</li> <li>Documentation System (MkDocs + Material) \u2190 This is it!</li> </ul>"},{"location":"#architecture-overview","title":"\ud83c\udfd7\ufe0f Architecture Overview","text":""},{"location":"#system-architecture","title":"System Architecture","text":"<pre><code>graph TB\n    subgraph \"BlackLake Platform\"\n        API[Rust API Server]\n        UI[React Web UI]\n        CLI[CLI Tool]\n        DB[(PostgreSQL)]\n        CACHE[(Redis)]\n        SEARCH[(Solr)]\n        STORAGE[S3 Storage]\n    end\n\n    API --&gt; DB\n    API --&gt; CACHE\n    API --&gt; SEARCH\n    API --&gt; STORAGE\n    UI --&gt; API\n    CLI --&gt; API</code></pre>"},{"location":"#technology-stack","title":"Technology Stack","text":"<ul> <li>Backend: Rust with Axum framework</li> <li>Frontend: React with TypeScript and Tailwind CSS</li> <li>Database: PostgreSQL with JSONB search</li> <li>Search: Apache Solr with advanced features</li> <li>Cache: Redis for sessions and job queues</li> <li>Storage: S3-compatible storage with MinIO</li> <li>Monitoring: Prometheus, Grafana, OpenTelemetry</li> <li>Containerization: Docker with multi-arch builds</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Multi-Tenant Architecture: ABAC policies with tenant isolation</li> <li>Advanced Security: OIDC/JWT authentication, RBAC, audit trails</li> <li>Comprehensive Governance: Branch protection, quotas, retention policies</li> <li>Production Operations: Monitoring, backup, disaster recovery</li> <li>Modern UI: React interface with mobile support and PWA capabilities</li> <li>Developer Experience: CLI tools, SDKs, comprehensive documentation</li> </ul>"},{"location":"#development-workflow","title":"\ud83d\udd27 Development Workflow","text":""},{"location":"#daily-development","title":"Daily Development","text":"<pre><code># Quick start\njust up-dev &amp;&amp; just logs api\n\n# With specific features\njust up-profiles \"dev,search-os\" &amp;&amp; just logs solr\n</code></pre>"},{"location":"#building-testing","title":"Building &amp; Testing","text":"<pre><code># Build all images\njust bake\n\n# Run tests\njust test\n\n# Lint and format\njust lint\njust fmt\n</code></pre>"},{"location":"#production-deployment","title":"Production Deployment","text":"<pre><code># Build production images\ndocker buildx bake prod\n\n# Deploy\njust up-prod\n</code></pre>"},{"location":"#monitoring-observability","title":"\ud83d\udcca Monitoring &amp; Observability","text":""},{"location":"#access-points","title":"Access Points","text":"<ul> <li>API: http://localhost:8080</li> <li>UI: http://localhost:8080 (served by gateway)</li> <li>Grafana: http://localhost:3000 (admin/admin)</li> <li>Solr: http://localhost:8983</li> <li>MinIO: http://localhost:9001 (minioadmin/minioadmin)</li> <li>Keycloak: http://localhost:8081 (admin/admin)</li> </ul>"},{"location":"#monitoring-stack","title":"Monitoring Stack","text":"<ul> <li>Prometheus: Metrics collection and storage</li> <li>Grafana: Dashboards and visualization</li> <li>OpenTelemetry: Distributed tracing</li> <li>Jaeger: Trace analysis and debugging</li> </ul>"},{"location":"#security-features","title":"\ud83d\udd12 Security Features","text":""},{"location":"#authentication-authorization","title":"Authentication &amp; Authorization","text":"<ul> <li>OIDC/JWT: Keycloak integration with JWKS rotation</li> <li>RBAC: Role-based access control</li> <li>ABAC: Attribute-based access control with policies</li> <li>Multi-Tenancy: Tenant isolation and data segregation</li> </ul>"},{"location":"#security-controls","title":"Security Controls","text":"<ul> <li>Input Validation: Comprehensive validation and sanitization</li> <li>Rate Limiting: Per-user and per-IP rate limiting</li> <li>Security Headers: CORS, CSRF protection, security headers</li> <li>Audit Logging: Complete audit trail for compliance</li> </ul>"},{"location":"#sdks-integrations","title":"\ud83d\udce6 SDKs &amp; Integrations","text":""},{"location":"#official-sdks","title":"Official SDKs","text":"<ul> <li>Python SDK: Full-featured Python client with async support</li> <li>TypeScript SDK: TypeScript client with type safety</li> <li>CLI Tools: Command-line interface for operations</li> </ul>"},{"location":"#federation-connectors","title":"Federation Connectors","text":"<ul> <li>S3 Connector: S3-compatible storage integration</li> <li>Postgres Connector: PostgreSQL database integration</li> <li>CKAN Connector: CKAN data portal integration</li> </ul>"},{"location":"#production-readiness","title":"\ud83d\ude80 Production Readiness","text":""},{"location":"#infrastructure","title":"Infrastructure","text":"<ul> <li>Container Orchestration: Docker Compose with profiles</li> <li>Multi-Architecture: AMD64/ARM64 builds</li> <li>Service Discovery: Internal networking</li> <li>Load Balancing: Gateway with Envoy/Nginx</li> <li>Health Monitoring: Comprehensive health checks</li> </ul>"},{"location":"#operations","title":"Operations","text":"<ul> <li>Monitoring: Prometheus metrics + Grafana dashboards</li> <li>Logging: Structured logging with OpenTelemetry</li> <li>Backup: Automated backup procedures</li> <li>Disaster Recovery: DR runbooks and procedures</li> <li>Performance: K6 load testing and monitoring</li> </ul>"},{"location":"#scalability","title":"Scalability","text":"<ul> <li>Horizontal Scaling: Stateless API design</li> <li>Database: Connection pooling and optimization</li> <li>Caching: Redis for sessions and job queues</li> <li>Search: Solr for advanced search capabilities</li> <li>Storage: S3-compatible with lifecycle policies</li> </ul>"},{"location":"#documentation-structure","title":"\ud83d\udcda Documentation Structure","text":"<p>This documentation is organized into the following sections:</p> <ul> <li>Project Status - Current project status and completion summary</li> <li>Verification - Comprehensive verification of all systems</li> <li>Fast Setup - Quick setup guide with build optimization</li> <li>Implementation Summary - Week-by-week implementation details</li> <li>Deployment - Production deployment guide</li> <li>Operations - Operations runbooks and procedures</li> <li>Search - Search functionality documentation</li> <li>Sessions - Session management documentation</li> <li>Images - Docker image documentation</li> </ul>"},{"location":"#getting-help","title":"\ud83c\udfaf Getting Help","text":""},{"location":"#documentation-issues","title":"Documentation Issues","text":"<p>If you find issues with the documentation, please: 1. Check the Project Status for current status 2. Review the Verification for system status 3. Consult the Fast Setup for setup issues</p>"},{"location":"#technical-issues","title":"Technical Issues","text":"<p>For technical issues: 1. Check the Operations runbooks 2. Review the Deployment guide 3. Consult the Implementation Summary for details</p>"},{"location":"#feature-requests","title":"Feature Requests","text":"<p>For feature requests and future development: 1. Check the TODO.md for carryover items 2. Review the Project Status for roadmap 3. Consult the Implementation Summary for current features</p>"},{"location":"#welcome-to-blacklake","title":"\ud83c\udf89 Welcome to BlackLake!","text":"<p>BlackLake is a production-ready, enterprise-grade data artifact management platform that combines modern technology with comprehensive features for data management, search, governance, and compliance.</p> <p>Ready to get started? Check out the Fast Setup guide and begin your journey with BlackLake!</p>"},{"location":"ACTIVE_STANDBY_GUIDE/","title":"Active-Standby Guide for BlackLake","text":""},{"location":"ACTIVE_STANDBY_GUIDE/#overview","title":"Overview","text":"<p>This guide provides comprehensive instructions for setting up and managing active-standby configurations for BlackLake API with database replication, including failover procedures and promotion protocols.</p>"},{"location":"ACTIVE_STANDBY_GUIDE/#architecture","title":"Architecture","text":""},{"location":"ACTIVE_STANDBY_GUIDE/#active-standby-setup","title":"Active-Standby Setup","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Active API    \u2502    \u2502  Standby API    \u2502\n\u2502   (Primary)     \u2502    \u2502  (Secondary)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502\n         \u2502                       \u2502\n         \u25bc                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Primary DB     \u2502\u25c4\u2500\u2500\u2500\u2524  Standby DB     \u2502\n\u2502  (Read/Write)   \u2502    \u2502  (Read Only)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ACTIVE_STANDBY_GUIDE/#components","title":"Components","text":"<ol> <li>Active API Server: Handles all read/write operations</li> <li>Standby API Server: Ready to take over in case of failure</li> <li>Primary Database: Master database with read/write access</li> <li>Standby Database: Replica database with read-only access</li> <li>Load Balancer: Routes traffic to active server</li> <li>Health Monitoring: Monitors system health and triggers failover</li> </ol>"},{"location":"ACTIVE_STANDBY_GUIDE/#setup-instructions","title":"Setup Instructions","text":""},{"location":"ACTIVE_STANDBY_GUIDE/#1-database-replication-setup","title":"1. Database Replication Setup","text":""},{"location":"ACTIVE_STANDBY_GUIDE/#postgresql-streaming-replication","title":"PostgreSQL Streaming Replication","text":"<pre><code># On Primary Database\n# Configure postgresql.conf\nwal_level = replica\nmax_wal_senders = 3\nmax_replication_slots = 3\nwal_keep_segments = 64\n\n# Configure pg_hba.conf\nhost replication replicator 10.0.0.0/8 md5\n\n# Create replication user\nsudo -u postgres psql\nCREATE USER replicator WITH REPLICATION ENCRYPTED PASSWORD 'replication_password';\n</code></pre>"},{"location":"ACTIVE_STANDBY_GUIDE/#standby-database-configuration","title":"Standby Database Configuration","text":"<pre><code># On Standby Database\n# Create base backup\npg_basebackup -h primary_host -D /var/lib/postgresql/data -U replicator -v -P -W\n\n# Configure recovery.conf\nstandby_mode = 'on'\nprimary_conninfo = 'host=primary_host port=5432 user=replicator password=replication_password'\ntrigger_file = '/tmp/postgresql.trigger'\n</code></pre>"},{"location":"ACTIVE_STANDBY_GUIDE/#2-api-server-configuration","title":"2. API Server Configuration","text":""},{"location":"ACTIVE_STANDBY_GUIDE/#active-server-configuration","title":"Active Server Configuration","text":"<pre><code># docker-compose.active.yml\nversion: '3.8'\nservices:\n  api:\n    image: blacklake/api:latest\n    environment:\n      - DATABASE_URL=postgresql://user:pass@primary_db:5432/blacklake\n      - REDIS_URL=redis://redis:6379\n      - ROLE=active\n    ports:\n      - \"8080:8080\"\n    depends_on:\n      - primary_db\n      - redis\n\n  primary_db:\n    image: postgres:15\n    environment:\n      - POSTGRES_DB=blacklake\n      - POSTGRES_USER=user\n      - POSTGRES_PASSWORD=pass\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n</code></pre>"},{"location":"ACTIVE_STANDBY_GUIDE/#standby-server-configuration","title":"Standby Server Configuration","text":"<pre><code># docker-compose.standby.yml\nversion: '3.8'\nservices:\n  api:\n    image: blacklake/api:latest\n    environment:\n      - DATABASE_URL=postgresql://user:pass@standby_db:5432/blacklake\n      - REDIS_URL=redis://redis:6379\n      - ROLE=standby\n    ports:\n      - \"8081:8080\"\n    depends_on:\n      - standby_db\n      - redis\n\n  standby_db:\n    image: postgres:15\n    environment:\n      - POSTGRES_DB=blacklake\n      - POSTGRES_USER=user\n      - POSTGRES_PASSWORD=pass\n    volumes:\n      - postgres_standby_data:/var/lib/postgresql/data\n</code></pre>"},{"location":"ACTIVE_STANDBY_GUIDE/#3-load-balancer-configuration","title":"3. Load Balancer Configuration","text":""},{"location":"ACTIVE_STANDBY_GUIDE/#nginx-configuration","title":"Nginx Configuration","text":"<pre><code># /etc/nginx/sites-available/blacklake\nupstream blacklake_backend {\n    server active_api:8080 max_fails=3 fail_timeout=30s;\n    server standby_api:8080 backup;\n}\n\nserver {\n    listen 80;\n    server_name blacklake.example.com;\n\n    location / {\n        proxy_pass http://blacklake_backend;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n\n        # Health check configuration\n        proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;\n        proxy_next_upstream_tries 2;\n        proxy_next_upstream_timeout 10s;\n    }\n\n    location /health {\n        proxy_pass http://blacklake_backend/health;\n        access_log off;\n    }\n}\n</code></pre>"},{"location":"ACTIVE_STANDBY_GUIDE/#failover-procedures","title":"Failover Procedures","text":""},{"location":"ACTIVE_STANDBY_GUIDE/#1-automatic-failover","title":"1. Automatic Failover","text":""},{"location":"ACTIVE_STANDBY_GUIDE/#health-check-configuration","title":"Health Check Configuration","text":"<pre><code># health-check.yml\nversion: '3.8'\nservices:\n  health-monitor:\n    image: blacklake/health-monitor:latest\n    environment:\n      - ACTIVE_API_URL=http://active_api:8080\n      - STANDBY_API_URL=http://standby_api:8080\n      - FAILOVER_THRESHOLD=3\n      - CHECK_INTERVAL=10s\n    volumes:\n      - ./failover-script.sh:/scripts/failover.sh\n</code></pre>"},{"location":"ACTIVE_STANDBY_GUIDE/#failover-script","title":"Failover Script","text":"<pre><code>#!/bin/bash\n# failover-script.sh\n\nACTIVE_API=\"http://active_api:8080\"\nSTANDBY_API=\"http://standby_api:8080\"\nFAILOVER_THRESHOLD=3\nCHECK_INTERVAL=10\n\ncheck_health() {\n    local api_url=$1\n    curl -f -s \"$api_url/health\" &gt; /dev/null\n    return $?\n}\n\nfailover() {\n    echo \"Initiating failover to standby server...\"\n\n    # Update load balancer configuration\n    sed -i 's/server active_api:8080/server standby_api:8080/' /etc/nginx/sites-available/blacklake\n    nginx -s reload\n\n    # Promote standby database\n    ssh standby_db \"touch /tmp/postgresql.trigger\"\n\n    # Update API configurations\n    docker-compose -f docker-compose.standby.yml up -d\n\n    echo \"Failover completed\"\n}\n\n# Main monitoring loop\nfailure_count=0\nwhile true; do\n    if ! check_health $ACTIVE_API; then\n        failure_count=$((failure_count + 1))\n        echo \"Health check failed. Count: $failure_count\"\n\n        if [ $failure_count -ge $FAILOVER_THRESHOLD ]; then\n            failover\n            break\n        fi\n    else\n        failure_count=0\n    fi\n\n    sleep $CHECK_INTERVAL\ndone\n</code></pre>"},{"location":"ACTIVE_STANDBY_GUIDE/#2-manual-failover","title":"2. Manual Failover","text":""},{"location":"ACTIVE_STANDBY_GUIDE/#step-by-step-manual-failover","title":"Step-by-Step Manual Failover","text":"<ol> <li> <p>Verify Standby Status <pre><code># Check standby database replication lag\npsql -h standby_db -c \"SELECT pg_last_wal_receive_lsn(), pg_last_wal_replay_lsn();\"\n</code></pre></p> </li> <li> <p>Stop Active API <pre><code>docker-compose -f docker-compose.active.yml down\n</code></pre></p> </li> <li> <p>Promote Standby Database <pre><code># On standby database server\ntouch /tmp/postgresql.trigger\n</code></pre></p> </li> <li> <p>Update Load Balancer <pre><code># Update nginx configuration\nsed -i 's/server active_api:8080/server standby_api:8080/' /etc/nginx/sites-available/blacklake\nnginx -s reload\n</code></pre></p> </li> <li> <p>Start Standby API <pre><code>docker-compose -f docker-compose.standby.yml up -d\n</code></pre></p> </li> <li> <p>Verify Failover <pre><code>curl -f http://blacklake.example.com/health\n</code></pre></p> </li> </ol>"},{"location":"ACTIVE_STANDBY_GUIDE/#promotion-procedures","title":"Promotion Procedures","text":""},{"location":"ACTIVE_STANDBY_GUIDE/#1-standby-to-active-promotion","title":"1. Standby to Active Promotion","text":""},{"location":"ACTIVE_STANDBY_GUIDE/#pre-promotion-checklist","title":"Pre-Promotion Checklist","text":"<ul> <li> Verify standby database is up-to-date</li> <li> Check replication lag is minimal (&lt; 1 second)</li> <li> Ensure standby API is healthy</li> <li> Verify all services are running</li> <li> Test connectivity to all dependencies</li> </ul>"},{"location":"ACTIVE_STANDBY_GUIDE/#promotion-steps","title":"Promotion Steps","text":"<ol> <li> <p>Stop Active Services <pre><code>docker-compose -f docker-compose.active.yml down\n</code></pre></p> </li> <li> <p>Promote Database <pre><code># On standby database\ntouch /tmp/postgresql.trigger\n</code></pre></p> </li> <li> <p>Update Configuration <pre><code># Update environment variables\nexport ROLE=active\nexport DATABASE_URL=postgresql://user:pass@standby_db:5432/blacklake\n</code></pre></p> </li> <li> <p>Start Active Services <pre><code>docker-compose -f docker-compose.active.yml up -d\n</code></pre></p> </li> <li> <p>Update Load Balancer <pre><code># Point load balancer to new active server\nsed -i 's/server standby_api:8080/server active_api:8080/' /etc/nginx/sites-available/blacklake\nnginx -s reload\n</code></pre></p> </li> </ol>"},{"location":"ACTIVE_STANDBY_GUIDE/#2-post-promotion-validation","title":"2. Post-Promotion Validation","text":""},{"location":"ACTIVE_STANDBY_GUIDE/#health-checks","title":"Health Checks","text":"<pre><code># Check API health\ncurl -f http://active_api:8080/health\n\n# Check database connectivity\ncurl -f http://active_api:8080/health/database\n\n# Check storage connectivity\ncurl -f http://active_api:8080/health/storage\n\n# Check Redis connectivity\ncurl -f http://active_api:8080/health/redis\n</code></pre>"},{"location":"ACTIVE_STANDBY_GUIDE/#functional-tests","title":"Functional Tests","text":"<pre><code># Test basic API operations\ncurl -X GET http://active_api:8080/api/v1/repos\ncurl -X POST http://active_api:8080/api/v1/repos -d '{\"name\":\"test\"}'\ncurl -X GET http://active_api:8080/api/v1/repos/test\n</code></pre>"},{"location":"ACTIVE_STANDBY_GUIDE/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"ACTIVE_STANDBY_GUIDE/#1-health-monitoring","title":"1. Health Monitoring","text":""},{"location":"ACTIVE_STANDBY_GUIDE/#custom-health-endpoints","title":"Custom Health Endpoints","text":"<pre><code>// crates/api/src/health.rs\npub async fn cluster_health(\n    State(state): State&lt;HealthState&gt;,\n) -&gt; Result&lt;Json&lt;ClusterHealth&gt;, ApiError&gt; {\n    let mut health = ClusterHealth {\n        status: \"healthy\".to_string(),\n        primary: ServiceStatus {\n            status: \"active\".to_string(),\n            last_check: chrono::Utc::now(),\n            replication_lag: 0.0,\n        },\n        standby: ServiceStatus {\n            status: \"standby\".to_string(),\n            last_check: chrono::Utc::now(),\n            replication_lag: 0.0,\n        },\n    };\n\n    // Check primary database\n    match check_primary_database(&amp;state).await {\n        Ok(lag) =&gt; {\n            health.primary.replication_lag = lag;\n        }\n        Err(_) =&gt; {\n            health.primary.status = \"unhealthy\".to_string();\n            health.status = \"degraded\".to_string();\n        }\n    }\n\n    // Check standby database\n    match check_standby_database(&amp;state).await {\n        Ok(lag) =&gt; {\n            health.standby.replication_lag = lag;\n        }\n        Err(_) =&gt; {\n            health.standby.status = \"unhealthy\".to_string();\n        }\n    }\n\n    Ok(Json(health))\n}\n</code></pre>"},{"location":"ACTIVE_STANDBY_GUIDE/#2-replication-lag-monitoring","title":"2. Replication Lag Monitoring","text":"<pre><code>-- Check replication lag\nSELECT \n    client_addr,\n    state,\n    pg_wal_lsn_diff(pg_current_wal_lsn(), sent_lsn) AS sent_lag,\n    pg_wal_lsn_diff(pg_current_wal_lsn(), flush_lsn) AS flush_lag,\n    pg_wal_lsn_diff(pg_current_wal_lsn(), replay_lsn) AS replay_lag\nFROM pg_stat_replication;\n</code></pre>"},{"location":"ACTIVE_STANDBY_GUIDE/#3-alerting-configuration","title":"3. Alerting Configuration","text":"<pre><code># alertmanager.yml\nroute:\n  group_by: ['alertname']\n  group_wait: 10s\n  group_interval: 10s\n  repeat_interval: 1h\n  receiver: 'web.hook'\n\nreceivers:\n- name: 'web.hook'\n  webhook_configs:\n  - url: 'http://alertmanager:9093/api/v1/alerts'\n    send_resolved: true\n\n# Prometheus rules\ngroups:\n- name: blacklake.rules\n  rules:\n  - alert: HighReplicationLag\n    expr: pg_replication_lag &gt; 10\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"High replication lag detected\"\n      description: \"Replication lag is {{ $value }} seconds\"\n\n  - alert: DatabaseDown\n    expr: up{job=\"postgres\"} == 0\n    for: 1m\n    labels:\n      severity: critical\n    annotations:\n      summary: \"Database is down\"\n      description: \"Database {{ $labels.instance }} is not responding\"\n</code></pre>"},{"location":"ACTIVE_STANDBY_GUIDE/#recovery-procedures","title":"Recovery Procedures","text":""},{"location":"ACTIVE_STANDBY_GUIDE/#1-full-system-recovery","title":"1. Full System Recovery","text":"<ol> <li> <p>Restore from Backup <pre><code># Restore database\npg_restore -h localhost -U user -d blacklake /backup/blacklake_backup.sql\n\n# Restore storage\naws s3 sync s3://blacklake-backup/ s3://blacklake-storage/\n</code></pre></p> </li> <li> <p>Reconfigure Services <pre><code># Update configuration files\n# Start services\ndocker-compose up -d\n</code></pre></p> </li> <li> <p>Validate Recovery <pre><code># Run health checks\n# Test functionality\n# Verify data integrity\n</code></pre></p> </li> </ol>"},{"location":"ACTIVE_STANDBY_GUIDE/#2-partial-recovery","title":"2. Partial Recovery","text":"<ol> <li>Identify Affected Components</li> <li>Restore Specific Services</li> <li>Validate Partial Recovery</li> <li>Monitor System Health</li> </ol>"},{"location":"ACTIVE_STANDBY_GUIDE/#best-practices","title":"Best Practices","text":""},{"location":"ACTIVE_STANDBY_GUIDE/#1-regular-testing","title":"1. Regular Testing","text":"<ul> <li>Monthly Failover Tests: Test failover procedures monthly</li> <li>Quarterly DR Drills: Full disaster recovery testing</li> <li>Annual Recovery Testing: Complete system recovery testing</li> </ul>"},{"location":"ACTIVE_STANDBY_GUIDE/#2-monitoring","title":"2. Monitoring","text":"<ul> <li>Continuous Health Monitoring: 24/7 health checks</li> <li>Replication Lag Alerts: Immediate alerts for lag &gt; 5 seconds</li> <li>Performance Monitoring: Track system performance metrics</li> </ul>"},{"location":"ACTIVE_STANDBY_GUIDE/#3-documentation","title":"3. Documentation","text":"<ul> <li>Keep Documentation Updated: Regular updates to procedures</li> <li>Version Control: Track changes to configurations</li> <li>Runbook Maintenance: Keep runbooks current and tested</li> </ul>"},{"location":"ACTIVE_STANDBY_GUIDE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ACTIVE_STANDBY_GUIDE/#common-issues","title":"Common Issues","text":"<ol> <li>Replication Lag</li> <li>Check network connectivity</li> <li>Verify database configuration</li> <li> <p>Monitor system resources</p> </li> <li> <p>Failover Failures</p> </li> <li>Check service dependencies</li> <li>Verify configuration files</li> <li> <p>Test connectivity</p> </li> <li> <p>Data Inconsistency</p> </li> <li>Compare checksums</li> <li>Check replication status</li> <li>Verify backup integrity</li> </ol>"},{"location":"ACTIVE_STANDBY_GUIDE/#support-contacts","title":"Support Contacts","text":"<ul> <li>Primary DBA: dba@company.com</li> <li>System Administrator: sysadmin@company.com</li> <li>On-Call Engineer: +1-555-ONCALL</li> </ul>"},{"location":"CHANGELOG/","title":"BlackLake Changelog","text":"<p>This document tracks all completed features and implementations in the BlackLake data platform.</p>"},{"location":"CHANGELOG/#week-1-critical-infrastructure","title":"Week 1 - Critical Infrastructure \u2705","text":""},{"location":"CHANGELOG/#authentication-security","title":"Authentication &amp; Security","text":"<ul> <li>\u2705 JWT Verification: Implemented proper OIDC token validation</li> <li>\u2705 OIDC Integration: Added JWKS key rotation and caching</li> <li>\u2705 Rate Limiting: Implemented request rate limiting</li> <li>\u2705 Request Timeouts: Added timeout handling</li> <li>\u2705 Circuit Breaker: Implemented circuit breaker patterns</li> </ul>"},{"location":"CHANGELOG/#job-processing-system","title":"Job Processing System","text":"<ul> <li>\u2705 Apalis Framework: Integrated Redis-based job processing</li> <li>\u2705 Job Manager: Implemented comprehensive job management</li> <li>\u2705 Job Queues: Set up job queue processing</li> <li>\u2705 Antivirus Scans: Implemented ClamAV integration</li> <li>\u2705 CSV/Parquet Sampling: Added file sampling capabilities</li> <li>\u2705 ONNX Model Sniffing: Implemented model metadata extraction</li> <li>\u2705 RDF Generation: Added RDF metadata processing</li> <li>\u2705 Export Jobs: Implemented export functionality</li> <li>\u2705 Job Status: Added job status retrieval</li> <li>\u2705 Dead Letter Queues: Implemented failed job handling</li> <li>\u2705 Job Retry Logic: Added automatic retry mechanisms</li> </ul>"},{"location":"CHANGELOG/#solr-integration","title":"Solr Integration","text":"<ul> <li>\u2705 SolrClient: Implemented Solr document indexing</li> <li>\u2705 Document Updates: Added document update capabilities</li> <li>\u2705 Document Deletion: Implemented document removal</li> <li>\u2705 Search Queries: Added search functionality</li> <li>\u2705 Suggestions: Implemented search suggestions</li> <li>\u2705 Status Monitoring: Added Solr health checks</li> <li>\u2705 Reindex Jobs: Implemented full reindexing</li> </ul>"},{"location":"CHANGELOG/#week-2-core-api-features","title":"Week 2 - Core API Features \u2705","text":""},{"location":"CHANGELOG/#connector-operations","title":"Connector Operations","text":"<ul> <li>\u2705 Connector Cloning: Implemented connector duplication</li> <li>\u2705 Connection Testing: Added connector test functionality</li> <li>\u2705 Data Syncing: Implemented connector synchronization</li> <li>\u2705 Status Retrieval: Added connector status monitoring</li> <li>\u2705 Audit Logging: Implemented connector audit trails</li> </ul>"},{"location":"CHANGELOG/#compliance-features","title":"Compliance Features","text":"<ul> <li>\u2705 Retention Policies: Implemented data retention management</li> <li>\u2705 Legal Holds: Added legal hold functionality</li> <li>\u2705 Audit Logs: Implemented comprehensive audit logging</li> <li>\u2705 Compliance Exports: Added compliance report generation</li> <li>\u2705 Admin Role Checks: Implemented proper authorization</li> <li>\u2705 Policy Enforcement: Added policy validation</li> </ul>"},{"location":"CHANGELOG/#storage-operations","title":"Storage Operations","text":"<ul> <li>\u2705 S3 Configuration: Implemented production-ready S3 setup</li> <li>\u2705 Retry Logic: Added exponential backoff retry</li> <li>\u2705 Lifecycle Policies: Implemented cost optimization</li> <li>\u2705 Versioning: Added S3 versioning support</li> <li>\u2705 Encryption: Implemented server-side encryption</li> </ul>"},{"location":"CHANGELOG/#governance-webhooks","title":"Governance &amp; Webhooks","text":"<ul> <li>\u2705 Webhook Delivery: Implemented webhook delivery tracking</li> <li>\u2705 Database Queries: Added webhook history queries</li> <li>\u2705 Delivery Status: Implemented status monitoring</li> <li>\u2705 Retry Scheduling: Added webhook retry logic</li> <li>\u2705 Dead Letter Queue: Implemented failed webhook handling</li> </ul>"},{"location":"CHANGELOG/#week-3-ui-implementation","title":"Week 3 - UI Implementation \u2705","text":""},{"location":"CHANGELOG/#mobile-search-api","title":"Mobile Search API","text":"<ul> <li>\u2705 API Service: Created mobileSearchApi.ts</li> <li>\u2705 Search Integration: Implemented real API calls</li> <li>\u2705 Suggestions: Added search suggestions</li> <li>\u2705 Compliance: Implemented compliance API calls</li> <li>\u2705 Connectors: Added connector management API</li> </ul>"},{"location":"CHANGELOG/#mobile-ui-components","title":"Mobile UI Components","text":"<ul> <li>\u2705 Search Context: Updated MobileSearchContext.tsx</li> <li>\u2705 Search Store: Updated mobileSearch.ts store</li> <li>\u2705 Search Pages: Updated MobileSearchPage.tsx</li> <li>\u2705 Semantic Search: Updated MobileSemanticSearchPage.tsx</li> <li>\u2705 Compliance Page: Updated MobileCompliancePage.tsx</li> <li>\u2705 Connectors Page: Updated MobileAdminConnectorsPage.tsx</li> </ul>"},{"location":"CHANGELOG/#mobile-ui-features","title":"Mobile UI Features","text":"<ul> <li>\u2705 Pagination: Implemented search result pagination</li> <li>\u2705 File Viewer: Added file viewing capabilities</li> <li>\u2705 Download: Implemented file download functionality</li> <li>\u2705 Sharing: Added file sharing capabilities</li> <li>\u2705 Favorites: Implemented bookmark functionality</li> <li>\u2705 Notifications: Added toast notifications</li> <li>\u2705 Job Details: Implemented job detail viewing</li> </ul>"},{"location":"CHANGELOG/#week-4-infrastructure-operations","title":"Week 4 - Infrastructure Operations \u2705","text":""},{"location":"CHANGELOG/#database-operations","title":"Database Operations","text":"<ul> <li>\u2705 Connection Pooling: Implemented database connection pooling</li> <li>\u2705 Retry Logic: Added exponential backoff retry</li> <li>\u2705 Health Checks: Implemented database health monitoring</li> <li>\u2705 Circuit Breaker: Added circuit breaker patterns</li> <li>\u2705 Read Replicas: Implemented read replica support</li> </ul>"},{"location":"CHANGELOG/#session-management","title":"Session Management","text":"<ul> <li>\u2705 Redis Integration: Implemented Redis session storage</li> <li>\u2705 Session Statistics: Added active/expired session tracking</li> <li>\u2705 Session Monitoring: Implemented session health checks</li> </ul>"},{"location":"CHANGELOG/#export-functionality","title":"Export Functionality","text":"<ul> <li>\u2705 Tarball Creation: Implemented real tar.gz archive creation</li> <li>\u2705 File Verification: Added file integrity checks</li> <li>\u2705 Error Handling: Implemented comprehensive error handling</li> </ul>"},{"location":"CHANGELOG/#compliance-jobs","title":"Compliance Jobs","text":"<ul> <li>\u2705 CSV Export: Implemented real CSV export for audit logs</li> <li>\u2705 Legal Holds Export: Added legal holds CSV export</li> <li>\u2705 Compliance Reports: Implemented comprehensive compliance reporting</li> </ul>"},{"location":"CHANGELOG/#week-5-performance-optimization","title":"Week 5 - Performance Optimization \u2705","text":""},{"location":"CHANGELOG/#redis-caching","title":"Redis Caching","text":"<ul> <li>\u2705 Search Results: Implemented Redis caching for search results</li> <li>\u2705 Metadata Caching: Added metadata caching</li> <li>\u2705 Cache Statistics: Implemented cache monitoring</li> <li>\u2705 TTL Management: Added configurable cache TTLs</li> </ul>"},{"location":"CHANGELOG/#database-optimization","title":"Database Optimization","text":"<ul> <li>\u2705 Query Optimization: Implemented optimized database queries</li> <li>\u2705 Indexing: Added database indexing</li> <li>\u2705 Dynamic Filtering: Implemented dynamic query building</li> <li>\u2705 Pagination: Added efficient pagination</li> <li>\u2705 Query Timing: Implemented query performance monitoring</li> </ul>"},{"location":"CHANGELOG/#monitoring-metrics","title":"Monitoring &amp; Metrics","text":"<ul> <li>\u2705 System Metrics: Implemented comprehensive system monitoring</li> <li>\u2705 API Metrics: Added API performance metrics</li> <li>\u2705 Database Metrics: Implemented database monitoring</li> <li>\u2705 Cache Metrics: Added cache performance metrics</li> </ul>"},{"location":"CHANGELOG/#analytics-reporting","title":"Analytics &amp; Reporting","text":"<ul> <li>\u2705 Usage Analytics: Implemented usage tracking</li> <li>\u2705 Performance Analytics: Added performance analysis</li> <li>\u2705 Security Analytics: Implemented security monitoring</li> <li>\u2705 Report Generation: Added analytics report generation</li> </ul>"},{"location":"CHANGELOG/#performance-testing","title":"Performance Testing","text":"<ul> <li>\u2705 Load Testing: Implemented k6 load testing</li> <li>\u2705 Stress Testing: Added stress testing capabilities</li> <li>\u2705 Performance Benchmarks: Implemented performance benchmarking</li> </ul>"},{"location":"CHANGELOG/#final-implementation-phase-remaining-critical-stubs","title":"Final Implementation Phase - Remaining Critical Stubs \u2705","text":""},{"location":"CHANGELOG/#rdf-metadata-processing","title":"RDF Metadata Processing","text":"<ul> <li>\u2705 JSON-LD Conversion: Implemented JSON-LD conversion with Dublin Core mapping</li> <li>\u2705 Turtle Format: Added Turtle format conversion with RDF serialization</li> <li>\u2705 Subject IRI: Implemented proper subject IRI generation</li> <li>\u2705 S3 Storage: Added RDF file storage in S3 with proper content-type headers</li> </ul>"},{"location":"CHANGELOG/#clamav-virus-scanning","title":"ClamAV Virus Scanning","text":"<ul> <li>\u2705 Real-time Scanning: Implemented ClamAV daemon integration</li> <li>\u2705 S3 Integration: Added S3 file download and TCP communication</li> <li>\u2705 Scan Results: Implemented comprehensive scan result handling</li> <li>\u2705 Database Updates: Added scan result database updates</li> <li>\u2705 Quarantine: Implemented infected file quarantine handling</li> </ul>"},{"location":"CHANGELOG/#export-package-creation","title":"Export Package Creation","text":"<ul> <li>\u2705 Artifact Collection: Implemented complete artifact collection</li> <li>\u2705 Tarball Creation: Added metadata and blob file organization</li> <li>\u2705 Gzip Compression: Implemented efficient storage and transfer</li> <li>\u2705 S3 Upload: Added proper organization and presigned URL generation</li> <li>\u2705 Cleanup: Implemented temporary file cleanup and error handling</li> </ul>"},{"location":"CHANGELOG/#reindex-job-processing","title":"Reindex Job Processing","text":"<ul> <li>\u2705 Apalis Integration: Implemented asynchronous job processing</li> <li>\u2705 Job Enqueueing: Added proper job data structures</li> <li>\u2705 Batch Processing: Implemented efficient large-scale reindexing</li> <li>\u2705 Error Handling: Added comprehensive error handling</li> <li>\u2705 Status Tracking: Implemented job status monitoring</li> </ul>"},{"location":"CHANGELOG/#infrastructure-operations","title":"Infrastructure &amp; Operations \u2705","text":""},{"location":"CHANGELOG/#active-standby-disaster-recovery","title":"Active-Standby &amp; Disaster Recovery","text":"<ul> <li>\u2705 Database Replication: Implemented database replication setup</li> <li>\u2705 Failover Procedures: Added automated failover procedures</li> <li>\u2705 Health Endpoints: Implemented health check endpoints</li> <li>\u2705 Backup Validation: Added automated backup validation</li> <li>\u2705 Chaos Engineering: Implemented chaos engineering probes</li> </ul>"},{"location":"CHANGELOG/#cost-lifecycle-governance","title":"Cost &amp; Lifecycle Governance","text":"<ul> <li>\u2705 Cost Estimation: Implemented cost tracking and estimation</li> <li>\u2705 Usage Metering: Added usage metering and monitoring</li> <li>\u2705 Budget Alerts: Implemented budget alert system</li> <li>\u2705 Lifecycle Policies: Added automated lifecycle management</li> </ul>"},{"location":"CHANGELOG/#access-reviews-data-egress-controls","title":"Access Reviews &amp; Data Egress Controls","text":"<ul> <li>\u2705 Access Review System: Implemented quarterly access review system</li> <li>\u2705 Signed URL Constraints: Added IP CIDR restrictions and user agent pinning</li> <li>\u2705 Rate Limiting: Implemented time-based access controls</li> <li>\u2705 Audit Logging: Added comprehensive access audit logging</li> </ul>"},{"location":"CHANGELOG/#performance-baseline-load-testing","title":"Performance Baseline &amp; Load Testing","text":"<ul> <li>\u2705 k6 Load Testing: Implemented comprehensive load testing scenarios</li> <li>\u2705 Performance Reporting: Added performance regression detection</li> <li>\u2705 Benchmarking: Implemented performance baseline establishment</li> <li>\u2705 Monitoring: Added real-time performance monitoring</li> </ul>"},{"location":"CHANGELOG/#documentation-system","title":"Documentation System","text":"<ul> <li>\u2705 MkDocs: Implemented MkDocs with Material theme</li> <li>\u2705 Mermaid Diagrams: Added Mermaid diagram support</li> <li>\u2705 OpenAPI Integration: Implemented API documentation</li> <li>\u2705 CI for Docs: Added automated documentation building</li> <li>\u2705 Content Creation: Implemented comprehensive documentation</li> <li>\u2705 UI Help Integration: Added contextual help system</li> </ul>"},{"location":"CHANGELOG/#security-authentication","title":"Security &amp; Authentication","text":"<ul> <li>\u2705 CSRF Protection: Implemented CSRF protection mechanisms</li> <li>\u2705 API Key Authentication: Added API key authentication</li> <li>\u2705 Request Signing: Implemented request signing and verification</li> <li>\u2705 Security Testing: Added security testing and penetration testing</li> </ul>"},{"location":"CHANGELOG/#infrastructure-operations_1","title":"Infrastructure &amp; Operations","text":"<ul> <li>\u2705 Kubernetes Manifests: Implemented Kubernetes deployment manifests</li> <li>\u2705 Helm Charts: Added Helm chart support</li> <li>\u2705 Horizontal Pod Autoscaling: Implemented HPA configuration</li> <li>\u2705 Service Mesh: Added service mesh integration</li> <li>\u2705 Blue-Green Deployment: Implemented blue-green deployment</li> <li>\u2705 Automated Deployment: Added automated deployment pipelines</li> <li>\u2705 Rollback Procedures: Implemented automated rollback procedures</li> </ul> <p>Last Updated: 2024-01-15 Total Features Implemented: 150+ features across all categories Status: Production Ready \u2705</p>"},{"location":"DEPLOYMENT/","title":"BlackLake Deployment Guide","text":"<p>This document provides comprehensive deployment instructions for the BlackLake data platform.</p>"},{"location":"DEPLOYMENT/#production-deployment","title":"Production Deployment","text":""},{"location":"DEPLOYMENT/#docker-compose","title":"Docker Compose","text":"<pre><code># Production deployment\ndocker-compose -f docker-compose.prod.yml up -d\n\n# With custom configuration\nBLACKLAKE_DOMAIN=blacklake.example.com docker-compose -f docker-compose.prod.yml up -d\n</code></pre>"},{"location":"DEPLOYMENT/#kubernetes","title":"Kubernetes","text":"<pre><code># Deploy to Kubernetes\nkubectl apply -f k8s/\n\n# With Helm\nhelm install blacklake ./helm/blacklake -f helm/blacklake/values.yaml\n</code></pre>"},{"location":"DEPLOYMENT/#additional-resources","title":"Additional Resources","text":"<ul> <li>Local Testing Guide</li> <li>Fast Setup Guide</li> <li>Migration Setup</li> <li>Project Status</li> <li>Implementation Summary</li> </ul>"},{"location":"DEPLOYMENT/#support","title":"Support","text":"<ul> <li>Documentation: Documentation Home</li> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> </ul>"},{"location":"FAST_SETUP/","title":"BlackLake Fast Setup Guide","text":"<p>This guide provides a quick setup for the BlackLake data platform with optimized build configurations.</p>"},{"location":"FAST_SETUP/#prerequisites","title":"Prerequisites","text":"<ul> <li>Rust 1.70+</li> <li>Docker and Docker Compose</li> <li><code>just</code> command runner (optional, for dev commands)</li> </ul>"},{"location":"FAST_SETUP/#quick-start","title":"Quick Start","text":""},{"location":"FAST_SETUP/#1-clone-and-build","title":"1. Clone and Build","text":"<pre><code>git clone https://github.com/your-org/blacklake.git\ncd blacklake\n\n# Build all crates with optimizations\ncargo build --workspace --release\n\n# Run tests\ncargo test --workspace\n</code></pre>"},{"location":"FAST_SETUP/#2-start-services","title":"2. Start Services","text":"<pre><code># Start all services with optimized configuration\ndocker-compose up -d\n\n# Or use just (if installed)\njust up-dev\n</code></pre>"},{"location":"FAST_SETUP/#3-verify-setup","title":"3. Verify Setup","text":"<pre><code># Check all services are running\ndocker-compose ps\n\n# Check API health\ncurl http://localhost:8080/health\n\n# Check database health\ncurl http://localhost:8080/health/db\n\n# Check storage health\ncurl http://localhost:8080/health/storage\n</code></pre>"},{"location":"FAST_SETUP/#optimized-build-configuration","title":"Optimized Build Configuration","text":""},{"location":"FAST_SETUP/#cargotoml-optimizations","title":"Cargo.toml Optimizations","text":"<pre><code>[profile.release]\nopt-level = 3\nlto = true\ncodegen-units = 1\npanic = \"abort\"\nstrip = true\n</code></pre>"},{"location":"FAST_SETUP/#docker-build-optimizations","title":"Docker Build Optimizations","text":"<pre><code># Multi-stage build for smaller images\nFROM rust:1.70-slim as builder\nWORKDIR /app\nCOPY . .\nRUN cargo build --release\n\nFROM debian:bookworm-slim\nCOPY --from=builder /app/target/release/blacklake-api /usr/local/bin/\nCMD [\"blacklake-api\"]\n</code></pre>"},{"location":"FAST_SETUP/#environment-variables","title":"Environment Variables","text":"<pre><code># Performance optimizations\nRUST_LOG=info\nRUST_BACKTRACE=1\nDATABASE_POOL_SIZE=20\nREDIS_POOL_SIZE=10\nCACHE_TTL=3600\n</code></pre>"},{"location":"FAST_SETUP/#development-workflow","title":"Development Workflow","text":""},{"location":"FAST_SETUP/#using-just-commands","title":"Using Just Commands","text":"<pre><code># Show all available commands\njust --list\n\n# Start development environment\njust up-dev\n\n# Run tests\njust test\n\n# Build optimized release\njust build-release\n\n# Clean and rebuild\njust clean-build\n</code></pre>"},{"location":"FAST_SETUP/#hot-reload-development","title":"Hot Reload Development","text":"<pre><code># Start with hot reload\njust dev\n\n# Watch for changes\njust watch\n\n# Run specific tests\njust test-unit\njust test-integration\n</code></pre>"},{"location":"FAST_SETUP/#performance-tuning","title":"Performance Tuning","text":""},{"location":"FAST_SETUP/#database-optimization","title":"Database Optimization","text":"<pre><code># Set database connection pool size\nexport DATABASE_POOL_SIZE=20\n\n# Enable query logging\nexport DATABASE_LOG_QUERIES=true\n\n# Set connection timeout\nexport DATABASE_TIMEOUT=30s\n</code></pre>"},{"location":"FAST_SETUP/#redis-configuration","title":"Redis Configuration","text":"<pre><code># Set Redis pool size\nexport REDIS_POOL_SIZE=10\n\n# Set cache TTL\nexport CACHE_TTL=3600\n\n# Enable Redis clustering\nexport REDIS_CLUSTER=true\n</code></pre>"},{"location":"FAST_SETUP/#s3-storage-optimization","title":"S3 Storage Optimization","text":"<pre><code># Set S3 connection pool size\nexport S3_POOL_SIZE=5\n\n# Enable S3 multipart uploads\nexport S3_MULTIPART_THRESHOLD=100MB\n\n# Set S3 timeout\nexport S3_TIMEOUT=60s\n</code></pre>"},{"location":"FAST_SETUP/#monitoring-setup","title":"Monitoring Setup","text":""},{"location":"FAST_SETUP/#prometheus-configuration","title":"Prometheus Configuration","text":"<pre><code># prometheus.yml\nglobal:\n  scrape_interval: 15s\n\nscrape_configs:\n  - job_name: 'blacklake-api'\n    static_configs:\n      - targets: ['api:8080']\n</code></pre>"},{"location":"FAST_SETUP/#grafana-dashboards","title":"Grafana Dashboards","text":"<pre><code># Import dashboards\ncurl -X POST http://localhost:3000/api/dashboards/db \\\n  -H \"Content-Type: application/json\" \\\n  -d @docs/grafana/dashboards/system-overview.json\n</code></pre>"},{"location":"FAST_SETUP/#troubleshooting","title":"Troubleshooting","text":""},{"location":"FAST_SETUP/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Build Failures <pre><code># Clean and rebuild\ncargo clean\ncargo build --workspace\n</code></pre></p> </li> <li> <p>Database Connection Issues <pre><code># Check database status\ndocker-compose logs db\n\n# Reset database\ndocker-compose down -v\ndocker-compose up -d db\n</code></pre></p> </li> <li> <p>Storage Issues <pre><code># Check MinIO status\ndocker-compose logs minio\n\n# Reset storage\ndocker-compose down -v\ndocker-compose up -d minio\n</code></pre></p> </li> </ol>"},{"location":"FAST_SETUP/#performance-issues","title":"Performance Issues","text":"<ol> <li>Slow API Responses</li> <li>Check database connection pool</li> <li>Verify Redis connectivity</li> <li> <p>Monitor memory usage</p> </li> <li> <p>High Memory Usage</p> </li> <li>Reduce connection pool sizes</li> <li>Enable garbage collection</li> <li> <p>Monitor for memory leaks</p> </li> <li> <p>Database Performance</p> </li> <li>Check query performance</li> <li>Verify indexes</li> <li>Monitor connection pool</li> </ol>"},{"location":"FAST_SETUP/#production-deployment","title":"Production Deployment","text":""},{"location":"FAST_SETUP/#docker-compose-production","title":"Docker Compose Production","text":"<pre><code># Production deployment\ndocker-compose -f docker-compose.prod.yml up -d\n\n# With custom configuration\nBLACKLAKE_DOMAIN=blacklake.example.com docker-compose -f docker-compose.prod.yml up -d\n</code></pre>"},{"location":"FAST_SETUP/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<pre><code># Deploy to Kubernetes\nkubectl apply -f k8s/\n\n# With Helm\nhelm install blacklake ./helm/blacklake -f helm/blacklake/values.yaml\n</code></pre>"},{"location":"FAST_SETUP/#additional-resources","title":"Additional Resources","text":"<ul> <li>Local Testing Guide</li> <li>Migration Setup</li> <li>CLI Documentation</li> <li>Project Status</li> <li>Implementation Summary</li> </ul>"},{"location":"FAST_SETUP/#support","title":"Support","text":"<ul> <li>Documentation: Documentation Home</li> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> </ul>"},{"location":"IMAGES/","title":"BlackLake Multi-Architecture Images Guide","text":""},{"location":"IMAGES/#week-5-container-image-strategy-and-optimization","title":"Week 5: Container image strategy and optimization","text":""},{"location":"IMAGES/#overview","title":"Overview","text":"<p>BlackLake uses Docker Buildx and docker-bake.hcl to build multi-architecture container images supporting both AMD64 and ARM64 platforms. This guide covers the image strategy, build process, and optimization techniques.</p>"},{"location":"IMAGES/#image-architecture","title":"Image Architecture","text":""},{"location":"IMAGES/#supported-platforms","title":"Supported Platforms","text":"<ul> <li>linux/amd64: Intel/AMD x86_64 processors</li> <li>linux/arm64: ARM 64-bit processors (Apple Silicon, ARM servers)</li> </ul>"},{"location":"IMAGES/#image-components","title":"Image Components","text":""},{"location":"IMAGES/#core-images","title":"Core Images","text":"<ul> <li>api: BlackLake REST API server</li> <li>ui: React-based web interface</li> <li>gateway: Nginx reverse proxy</li> <li>jobrunner: Background job processor</li> </ul>"},{"location":"IMAGES/#supporting-images","title":"Supporting Images","text":"<ul> <li>otel-collector: OpenTelemetry data collection</li> <li>mlflow: ML experiment tracking (optional)</li> </ul>"},{"location":"IMAGES/#build-configuration","title":"Build Configuration","text":""},{"location":"IMAGES/#docker-bakehcl","title":"docker-bake.hcl","text":"<p>The build configuration is defined in <code>docker-bake.hcl</code>:</p> <pre><code># Common variables\nvariable \"REGISTRY\" {\n  default = \"ghcr.io\"\n}\n\nvariable \"IMAGE_PREFIX\" {\n  default = \"blacklake\"\n}\n\nvariable \"VERSION\" {\n  default = \"latest\"\n}\n\n# Common platforms\nvariable \"PLATFORMS\" {\n  default = [\"linux/amd64\", \"linux/arm64\"]\n}\n</code></pre>"},{"location":"IMAGES/#build-targets","title":"Build Targets","text":""},{"location":"IMAGES/#api-image","title":"API Image","text":"<pre><code>target \"api\" {\n  dockerfile = \"Dockerfile.api\"\n  context = \".\"\n  platforms = PLATFORMS\n  tags = [\n    \"${REGISTRY}/${IMAGE_PREFIX}/api:${VERSION}\",\n    \"${REGISTRY}/${IMAGE_PREFIX}/api:latest\"\n  ]\n}\n</code></pre>"},{"location":"IMAGES/#ui-image","title":"UI Image","text":"<pre><code>target \"ui\" {\n  dockerfile = \"Dockerfile.ui\"\n  context = \"./ui\"\n  platforms = PLATFORMS\n  tags = [\n    \"${REGISTRY}/${IMAGE_PREFIX}/ui:${VERSION}\",\n    \"${REGISTRY}/${IMAGE_PREFIX}/ui:latest\"\n  ]\n}\n</code></pre>"},{"location":"IMAGES/#dockerfile-strategies","title":"Dockerfile Strategies","text":""},{"location":"IMAGES/#multi-stage-builds","title":"Multi-Stage Builds","text":"<p>All images use multi-stage builds for optimization:</p>"},{"location":"IMAGES/#api-dockerfile","title":"API Dockerfile","text":"<pre><code># Planner stage\nFROM rust:1.75-slim as planner\nWORKDIR /app\nRUN cargo install cargo-chef\nCOPY . .\nRUN cargo chef prepare --recipe-path recipe.json\n\n# Builder stage\nFROM rust:1.75-slim as builder\nWORKDIR /app\nCOPY --from=planner /app/recipe.json recipe.json\nRUN cargo chef cook --release --recipe-path recipe.json\nCOPY . .\nRUN cargo build --release --bin api\n\n# Runtime stage\nFROM debian:bookworm-slim as runtime\nRUN apt-get update &amp;&amp; apt-get install -y ca-certificates libssl3 libpq5\nCOPY --from=builder /app/target/release/api /app/api\nUSER 1001\nEXPOSE 8080\nCMD [\"./api\"]\n</code></pre>"},{"location":"IMAGES/#ui-dockerfile","title":"UI Dockerfile","text":"<pre><code># Builder stage\nFROM node:20-alpine as builder\nRUN npm install -g pnpm\nWORKDIR /app\nCOPY package.json pnpm-lock.yaml ./\nRUN pnpm fetch\nCOPY . .\nRUN pnpm install --frozen-lockfile\nRUN pnpm build\n\n# Runtime stage\nFROM nginx:1.25-alpine as runtime\nCOPY --from=builder /app/dist /usr/share/nginx/html\nCOPY nginx.conf /etc/nginx/nginx.conf\nUSER 1001\nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n</code></pre>"},{"location":"IMAGES/#security-best-practices","title":"Security Best Practices","text":""},{"location":"IMAGES/#non-root-users","title":"Non-Root Users","text":"<pre><code># Create non-root user\nRUN groupadd -r blacklake &amp;&amp; useradd -r -g blacklake blacklake\nUSER blacklake\n</code></pre>"},{"location":"IMAGES/#minimal-base-images","title":"Minimal Base Images","text":"<ul> <li>API: <code>debian:bookworm-slim</code> (minimal Debian)</li> <li>UI: <code>nginx:1.25-alpine</code> (Alpine Linux)</li> <li>Gateway: <code>nginx:1.25-alpine</code></li> </ul>"},{"location":"IMAGES/#security-scanning","title":"Security Scanning","text":"<pre><code># Add security labels\nLABEL org.opencontainers.image.title=\"BlackLake API\"\nLABEL org.opencontainers.image.description=\"BlackLake Data Artifact Management API\"\nLABEL org.opencontainers.image.vendor=\"BlackLake\"\nLABEL org.opencontainers.image.licenses=\"MIT\"\n</code></pre>"},{"location":"IMAGES/#build-process","title":"Build Process","text":""},{"location":"IMAGES/#local-development","title":"Local Development","text":""},{"location":"IMAGES/#build-single-architecture","title":"Build Single Architecture","text":"<pre><code># Build for current platform\ndocker build -t blacklake-api:local .\n\n# Build for specific platform\ndocker build --platform linux/amd64 -t blacklake-api:amd64 .\n</code></pre>"},{"location":"IMAGES/#build-multi-architecture","title":"Build Multi-Architecture","text":"<pre><code># Build all targets locally\njust bake\n\n# Build specific target\njust bake-target api\n\n# Build for specific platform\njust bake-platform linux/arm64 api\n</code></pre>"},{"location":"IMAGES/#cicd-pipeline","title":"CI/CD Pipeline","text":""},{"location":"IMAGES/#github-actions","title":"GitHub Actions","text":"<pre><code>- name: Set up Docker Buildx\n  uses: docker/setup-buildx-action@v3\n\n- name: Build and push\n  uses: docker/build-push-action@v5\n  with:\n    context: .\n    platforms: linux/amd64,linux/arm64\n    push: true\n    tags: ghcr.io/blacklake/api:latest\n    cache-from: type=gha,scope=api\n    cache-to: type=gha,mode=max,scope=api\n</code></pre>"},{"location":"IMAGES/#build-commands","title":"Build Commands","text":"<pre><code># Build and push all images\njust bake-push\n\n# Build specific target\ndocker buildx bake --push api\n\n# Build with custom registry\nREGISTRY=my-registry.com IMAGE_PREFIX=myorg docker buildx bake --push all\n</code></pre>"},{"location":"IMAGES/#caching-strategy","title":"Caching Strategy","text":""},{"location":"IMAGES/#build-cache","title":"Build Cache","text":""},{"location":"IMAGES/#registry-cache","title":"Registry Cache","text":"<pre><code>function \"cache_config\" {\n  returns = {\n    \"cache-from\" = [\n      \"type=gha,scope=${IMAGE_PREFIX}-${REGISTRY}\",\n      \"type=local,src=/tmp/.buildx-cache\"\n    ]\n    \"cache-to\" = [\n      \"type=gha,mode=max,scope=${IMAGE_PREFIX}-${REGISTRY}\",\n      \"type=local,dest=/tmp/.buildx-cache-new,mode=max\"\n    ]\n  }\n}\n</code></pre>"},{"location":"IMAGES/#layer-caching","title":"Layer Caching","text":"<pre><code># Copy dependency files first\nCOPY Cargo.toml Cargo.lock ./\nRUN cargo fetch\n\n# Copy source code last\nCOPY src/ ./src/\nRUN cargo build --release\n</code></pre>"},{"location":"IMAGES/#runtime-cache","title":"Runtime Cache","text":""},{"location":"IMAGES/#package-manager-cache","title":"Package Manager Cache","text":"<pre><code># Use pnpm fetch for better caching\nCOPY package.json pnpm-lock.yaml ./\nRUN pnpm fetch\nCOPY . .\nRUN pnpm install --frozen-lockfile\n</code></pre>"},{"location":"IMAGES/#build-cache_1","title":"Build Cache","text":"<pre><code># Use cargo-chef for Rust builds\nCOPY --from=planner /app/recipe.json recipe.json\nRUN cargo chef cook --release --recipe-path recipe.json\n</code></pre>"},{"location":"IMAGES/#image-optimization","title":"Image Optimization","text":""},{"location":"IMAGES/#size-optimization","title":"Size Optimization","text":""},{"location":"IMAGES/#multi-stage-builds_1","title":"Multi-Stage Builds","text":"<ul> <li>Separate build and runtime stages</li> <li>Remove build dependencies in runtime</li> <li>Use minimal base images</li> </ul>"},{"location":"IMAGES/#layer-optimization","title":"Layer Optimization","text":"<pre><code># Combine RUN commands\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y ca-certificates libssl3 &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n# Use .dockerignore\necho \"target/\" &gt;&gt; .dockerignore\necho \"node_modules/\" &gt;&gt; .dockerignore\n</code></pre>"},{"location":"IMAGES/#compression","title":"Compression","text":"<pre><code># Use distroless images for production\nFROM gcr.io/distroless/cc-debian12\nCOPY --from=builder /app/api /api\nENTRYPOINT [\"/api\"]\n</code></pre>"},{"location":"IMAGES/#performance-optimization","title":"Performance Optimization","text":""},{"location":"IMAGES/#build-performance","title":"Build Performance","text":"<ul> <li>Use BuildKit features</li> <li>Parallel builds</li> <li>Cache optimization</li> <li>Incremental builds</li> </ul>"},{"location":"IMAGES/#runtime-performance","title":"Runtime Performance","text":"<ul> <li>Optimized base images</li> <li>Minimal attack surface</li> <li>Fast startup times</li> <li>Efficient resource usage</li> </ul>"},{"location":"IMAGES/#registry-management","title":"Registry Management","text":""},{"location":"IMAGES/#image-tagging","title":"Image Tagging","text":""},{"location":"IMAGES/#semantic-versioning","title":"Semantic Versioning","text":"<pre><code># Release tags\nghcr.io/blacklake/api:v1.2.3\nghcr.io/blacklake/api:v1.2\nghcr.io/blacklake/api:v1\nghcr.io/blacklake/api:latest\n</code></pre>"},{"location":"IMAGES/#development-tags","title":"Development Tags","text":"<pre><code># Branch tags\nghcr.io/blacklake/api:main\nghcr.io/blacklake/api:develop\nghcr.io/blacklake/api:feature-branch\n\n# Commit tags\nghcr.io/blacklake/api:sha-abc123\nghcr.io/blacklake/api:pr-123\n</code></pre>"},{"location":"IMAGES/#registry-configuration","title":"Registry Configuration","text":""},{"location":"IMAGES/#github-container-registry","title":"GitHub Container Registry","text":"<pre><code># .github/workflows/build.yml\nenv:\n  REGISTRY: ghcr.io\n  IMAGE_PREFIX: blacklake\n</code></pre>"},{"location":"IMAGES/#custom-registry","title":"Custom Registry","text":"<pre><code># Configure custom registry\nexport REGISTRY=my-registry.com\nexport IMAGE_PREFIX=myorg\ndocker buildx bake --push all\n</code></pre>"},{"location":"IMAGES/#security-and-compliance","title":"Security and Compliance","text":""},{"location":"IMAGES/#image-security","title":"Image Security","text":""},{"location":"IMAGES/#vulnerability-scanning","title":"Vulnerability Scanning","text":"<pre><code># Scan with Trivy\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock \\\n  aquasec/trivy image blacklake/api:latest\n\n# Scan with Grype\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock \\\n  anchore/grype blacklake/api:latest\n</code></pre>"},{"location":"IMAGES/#sbom-generation","title":"SBOM Generation","text":"<pre><code># Generate SBOM\ndocker buildx build --sbom=generator=image --push .\n</code></pre>"},{"location":"IMAGES/#provenance","title":"Provenance","text":"<pre><code># Generate provenance\ndocker buildx build --provenance=mode=max --push .\n</code></pre>"},{"location":"IMAGES/#compliance","title":"Compliance","text":""},{"location":"IMAGES/#oci-compliance","title":"OCI Compliance","text":"<ul> <li>Follow OCI image specification</li> <li>Use standard labels</li> <li>Implement proper metadata</li> </ul>"},{"location":"IMAGES/#security-standards","title":"Security Standards","text":"<ul> <li>Non-root users</li> <li>Minimal base images</li> <li>Regular updates</li> <li>Vulnerability scanning</li> </ul>"},{"location":"IMAGES/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"IMAGES/#image-metrics","title":"Image Metrics","text":""},{"location":"IMAGES/#build-metrics","title":"Build Metrics","text":"<ul> <li>Build time</li> <li>Image size</li> <li>Layer count</li> <li>Cache hit rate</li> </ul>"},{"location":"IMAGES/#runtime-metrics","title":"Runtime Metrics","text":"<ul> <li>Startup time</li> <li>Memory usage</li> <li>CPU usage</li> <li>Network performance</li> </ul>"},{"location":"IMAGES/#health-checks","title":"Health Checks","text":""},{"location":"IMAGES/#container-health","title":"Container Health","text":"<pre><code>HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n  CMD curl -f http://localhost:8080/live || exit 1\n</code></pre>"},{"location":"IMAGES/#application-health","title":"Application Health","text":"<pre><code>// Health check endpoint\nasync fn health_check() -&gt; impl IntoResponse {\n    Json(serde_json::json!({\n        \"status\": \"healthy\",\n        \"timestamp\": chrono::Utc::now()\n    }))\n}\n</code></pre>"},{"location":"IMAGES/#troubleshooting","title":"Troubleshooting","text":""},{"location":"IMAGES/#build-issues","title":"Build Issues","text":""},{"location":"IMAGES/#platform-specific-builds","title":"Platform-Specific Builds","text":"<pre><code># Check available platforms\ndocker buildx ls\n\n# Create new builder\ndocker buildx create --name multiarch --use\n\n# Inspect builder\ndocker buildx inspect multiarch\n</code></pre>"},{"location":"IMAGES/#cache-issues","title":"Cache Issues","text":"<pre><code># Clear build cache\ndocker buildx prune\n\n# Clear registry cache\ndocker buildx prune --filter type=registry\n</code></pre>"},{"location":"IMAGES/#runtime-issues","title":"Runtime Issues","text":""},{"location":"IMAGES/#architecture-mismatch","title":"Architecture Mismatch","text":"<pre><code># Check image architecture\ndocker inspect blacklake/api:latest | jq '.[0].Architecture'\n\n# Run on specific platform\ndocker run --platform linux/amd64 blacklake/api:latest\n</code></pre>"},{"location":"IMAGES/#performance-issues","title":"Performance Issues","text":"<pre><code># Profile container performance\ndocker stats blacklake-api\n\n# Check resource usage\ndocker exec blacklake-api top\n</code></pre>"},{"location":"IMAGES/#best-practices","title":"Best Practices","text":""},{"location":"IMAGES/#development","title":"Development","text":"<ol> <li>Use multi-stage builds for smaller images</li> <li>Cache dependencies to speed up builds</li> <li>Use .dockerignore to exclude unnecessary files</li> <li>Test on multiple platforms before release</li> </ol>"},{"location":"IMAGES/#production","title":"Production","text":"<ol> <li>Use distroless images for security</li> <li>Implement health checks for monitoring</li> <li>Use semantic versioning for tags</li> <li>Scan for vulnerabilities regularly</li> </ol>"},{"location":"IMAGES/#cicd","title":"CI/CD","text":"<ol> <li>Use build cache to speed up builds</li> <li>Build multi-arch in parallel</li> <li>Sign images for security</li> <li>Generate SBOM for compliance</li> </ol>"},{"location":"IMAGES/#references","title":"References","text":"<ul> <li>Docker Buildx Documentation</li> <li>Multi-Architecture Builds</li> <li>OCI Image Specification</li> <li>BlackLake Deployment Guide</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/","title":"BlackLake Implementation Summary","text":"<p>This document provides a comprehensive summary of the BlackLake data platform implementation across all phases.</p>"},{"location":"IMPLEMENTATION_SUMMARY/#implementation-overview","title":"Implementation Overview","text":"<p>The BlackLake data platform has been fully implemented with 150+ features across 5 major phases, resulting in a production-ready system.</p>"},{"location":"IMPLEMENTATION_SUMMARY/#phase-1-critical-infrastructure-week-1","title":"Phase 1: Critical Infrastructure (Week 1) \u2705","text":""},{"location":"IMPLEMENTATION_SUMMARY/#authentication-security","title":"Authentication &amp; Security","text":"<ul> <li>JWT Verification, OIDC Integration, Rate Limiting, Request Timeouts, Circuit Breaker</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#job-processing-system","title":"Job Processing System","text":"<ul> <li>Apalis Framework, Job Manager, Job Queues, Antivirus Scans, CSV/Parquet Sampling, ONNX Model Sniffing, RDF Generation, Export Jobs, Job Status, Dead Letter Queues, Job Retry Logic</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#solr-integration","title":"Solr Integration","text":"<ul> <li>SolrClient, Document Updates, Document Deletion, Search Queries, Suggestions, Status Monitoring, Reindex Jobs</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#phase-2-core-api-features-week-2","title":"Phase 2: Core API Features (Week 2) \u2705","text":""},{"location":"IMPLEMENTATION_SUMMARY/#connector-operations","title":"Connector Operations","text":"<ul> <li>Connector Cloning, Connection Testing, Data Syncing, Status Retrieval, Audit Logging</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#compliance-features","title":"Compliance Features","text":"<ul> <li>Retention Policies, Legal Holds, Audit Logs, Compliance Exports, Admin Role Checks, Policy Enforcement</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#storage-operations","title":"Storage Operations","text":"<ul> <li>S3 Configuration, Retry Logic, Lifecycle Policies, Versioning, Encryption</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#governance-webhooks","title":"Governance &amp; Webhooks","text":"<ul> <li>Webhook Delivery, Database Queries, Delivery Status, Retry Scheduling, Dead Letter Queue</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#phase-3-ui-implementation-week-3","title":"Phase 3: UI Implementation (Week 3) \u2705","text":""},{"location":"IMPLEMENTATION_SUMMARY/#mobile-search-api","title":"Mobile Search API","text":"<ul> <li>API Service, Search Integration, Suggestions, Compliance, Connectors</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#mobile-ui-components","title":"Mobile UI Components","text":"<ul> <li>Search Context, Search Store, Search Pages, Semantic Search, Compliance Page, Connectors Page</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#mobile-ui-features","title":"Mobile UI Features","text":"<ul> <li>Pagination, File Viewer, Download, Sharing, Favorites, Notifications, Job Details</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#phase-4-infrastructure-operations-week-4","title":"Phase 4: Infrastructure Operations (Week 4) \u2705","text":""},{"location":"IMPLEMENTATION_SUMMARY/#database-operations","title":"Database Operations","text":"<ul> <li>Connection Pooling, Retry Logic, Health Checks, Circuit Breaker, Read Replicas</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#session-management","title":"Session Management","text":"<ul> <li>Redis Integration, Session Statistics, Session Monitoring</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#export-functionality","title":"Export Functionality","text":"<ul> <li>Tarball Creation, File Verification, Error Handling</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#compliance-jobs","title":"Compliance Jobs","text":"<ul> <li>CSV Export, Legal Holds Export, Compliance Reports</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#phase-5-performance-optimization-week-5","title":"Phase 5: Performance Optimization (Week 5) \u2705","text":""},{"location":"IMPLEMENTATION_SUMMARY/#redis-caching","title":"Redis Caching","text":"<ul> <li>Search Results, Metadata Caching, Cache Statistics, TTL Management</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#database-optimization","title":"Database Optimization","text":"<ul> <li>Query Optimization, Indexing, Dynamic Filtering, Pagination, Query Timing</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#monitoring-metrics","title":"Monitoring &amp; Metrics","text":"<ul> <li>System Metrics, API Metrics, Database Metrics, Cache Metrics</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#analytics-reporting","title":"Analytics &amp; Reporting","text":"<ul> <li>Usage Analytics, Performance Analytics, Security Analytics, Report Generation</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#performance-testing","title":"Performance Testing","text":"<ul> <li>Load Testing, Stress Testing, Performance Benchmarks</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#final-implementation-phase-remaining-critical-stubs","title":"Final Implementation Phase - Remaining Critical Stubs \u2705","text":""},{"location":"IMPLEMENTATION_SUMMARY/#rdf-metadata-processing","title":"RDF Metadata Processing","text":"<ul> <li>JSON-LD Conversion, Turtle Format, Subject IRI, S3 Storage</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#clamav-virus-scanning","title":"ClamAV Virus Scanning","text":"<ul> <li>Real-time Scanning, S3 Integration, Scan Results, Database Updates, Quarantine</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#export-package-creation","title":"Export Package Creation","text":"<ul> <li>Artifact Collection, Tarball Creation, Gzip Compression, S3 Upload, Cleanup</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#reindex-job-processing","title":"Reindex Job Processing","text":"<ul> <li>Apalis Integration, Job Enqueueing, Batch Processing, Error Handling, Status Tracking</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#infrastructure-operations","title":"Infrastructure &amp; Operations \u2705","text":""},{"location":"IMPLEMENTATION_SUMMARY/#active-standby-disaster-recovery","title":"Active-Standby &amp; Disaster Recovery","text":"<ul> <li>Database Replication, Failover Procedures, Health Endpoints, Backup Validation, Chaos Engineering</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#cost-lifecycle-governance","title":"Cost &amp; Lifecycle Governance","text":"<ul> <li>Cost Estimation, Usage Metering, Budget Alerts, Lifecycle Policies</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#access-reviews-data-egress-controls","title":"Access Reviews &amp; Data Egress Controls","text":"<ul> <li>Access Review System, Signed URL Constraints, Rate Limiting, Audit Logging</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#performance-baseline-load-testing","title":"Performance Baseline &amp; Load Testing","text":"<ul> <li>k6 Load Testing, Performance Reporting, Benchmarking, Monitoring</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#documentation-system","title":"Documentation System","text":"<ul> <li>MkDocs, Mermaid Diagrams, OpenAPI Integration, CI for Docs, Content Creation, UI Help Integration</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#security-authentication","title":"Security &amp; Authentication","text":"<ul> <li>CSRF Protection, API Key Authentication, Request Signing, Security Testing</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#infrastructure-operations_1","title":"Infrastructure &amp; Operations","text":"<ul> <li>Kubernetes Manifests, Helm Charts, Horizontal Pod Autoscaling, Service Mesh, Blue-Green Deployment, Automated Deployment, Rollback Procedures</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#conclusion","title":"Conclusion","text":"<p>The BlackLake data platform is production-ready with comprehensive features, robust architecture, and extensive testing. All critical components have been implemented and tested, providing a solid foundation for data artifact management and ML operations.</p> <p>Status: \u2705 Production Ready Total Features: 150+ features implemented Last Updated: 2024-01-15 Next Review: 2024-02-15</p>"},{"location":"MIGRATION_SETUP/","title":"BlackLake Migration and CLI Setup","text":"<p>This document explains how to handle database migrations and use the BlackLake CLI in the Docker Compose setup.</p>"},{"location":"MIGRATION_SETUP/#database-migrations","title":"\ud83d\uddc4\ufe0f Database Migrations","text":""},{"location":"MIGRATION_SETUP/#how-migrations-work","title":"How Migrations Work","text":"<p>The migration system is designed to run before any application services start, ensuring the database schema is always up-to-date.</p>"},{"location":"MIGRATION_SETUP/#migration-files","title":"Migration Files","text":"<p>All migration files are located in the <code>migrations/</code> directory:</p> <ul> <li><code>0001_init.sql</code> - Initial database schema</li> <li><code>0002_metadata_index_and_rdf.sql</code> - Metadata and RDF support</li> <li><code>0003_governance_features.sql</code> - Governance features</li> <li><code>0003_lineage_and_quotas.sql</code> - Lineage and quota management</li> <li><code>0004_multitenant_abac.sql</code> - Multi-tenant access control</li> <li><code>0005_data_classification.sql</code> - Data classification</li> <li><code>0006_external_sources.sql</code> - External data sources</li> <li><code>0007_compliance_features.sql</code> - Compliance features</li> <li><code>0008_compliance_jobs.sql</code> - Compliance job processing</li> <li><code>0009_fix_missing_columns.sql</code> - Schema fixes</li> <li><code>0010_api_missing_tables.sql</code> - API-specific tables</li> </ul>"},{"location":"MIGRATION_SETUP/#running-migrations","title":"Running Migrations","text":""},{"location":"MIGRATION_SETUP/#option-1-automatic-recommended","title":"Option 1: Automatic (Recommended)","text":"<p>Migrations run automatically when you start the services:</p> <pre><code># Start all services (migrations run first)\ndocker-compose up -d\n\n# Start with specific profiles\ndocker-compose --profile dev up -d\n</code></pre>"},{"location":"MIGRATION_SETUP/#option-2-manual-migration","title":"Option 2: Manual Migration","text":"<p>Run migrations manually:</p> <pre><code># Run migrations only\ndocker-compose run --rm migrations\n\n# Or use the migration script directly\ndocker-compose run --rm migrations /app/scripts/run-migrations.sh\n</code></pre>"},{"location":"MIGRATION_SETUP/#option-3-one-time-migration","title":"Option 3: One-time Migration","text":"<p>For production deployments:</p> <pre><code># Run migrations and exit\ndocker-compose run --rm migrate\n</code></pre>"},{"location":"MIGRATION_SETUP/#migration-dependencies","title":"Migration Dependencies","text":"<p>The migration system ensures proper ordering:</p> <ol> <li>Database starts first</li> <li>Migrations run after database is healthy</li> <li>API and other services start after migrations complete</li> </ol>"},{"location":"MIGRATION_SETUP/#blacklake-cli","title":"\ud83d\udda5\ufe0f BlackLake CLI","text":""},{"location":"MIGRATION_SETUP/#cli-service","title":"CLI Service","text":"<p>The CLI service provides an interactive shell with the BlackLake CLI pre-installed.</p>"},{"location":"MIGRATION_SETUP/#using-the-cli","title":"Using the CLI","text":""},{"location":"MIGRATION_SETUP/#start-cli-service","title":"Start CLI Service","text":"<pre><code># Start CLI service\ndocker-compose up cli\n\n# Or run interactively\ndocker-compose run --rm cli\n</code></pre>"},{"location":"MIGRATION_SETUP/#cli-commands","title":"CLI Commands","text":"<p>Once in the CLI container, you can use all BlackLake CLI commands:</p> <pre><code># List repositories\nblacklake-cli repos list\n\n# Search for files\nblacklake-cli search --query \"documentation\"\n\n# Upload a file\nblacklake-cli put --file /data/myfile.txt --repo my-repo\n\n# Get repository info\nblacklake-cli repos get my-repo\n</code></pre>"},{"location":"MIGRATION_SETUP/#cli-environment","title":"CLI Environment","text":"<p>The CLI service is configured with:</p> <ul> <li>Working Directory: <code>/data</code> (mounted from <code>./data</code>)</li> <li>Database Access: Connected to the main database</li> <li>S3 Access: Connected to MinIO</li> <li>API Access: Connected to the API service</li> </ul>"},{"location":"MIGRATION_SETUP/#volume-mounts","title":"Volume Mounts","text":"<p>The CLI service mounts: - <code>./data:/data</code> - Your local data directory</p>"},{"location":"MIGRATION_SETUP/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"MIGRATION_SETUP/#1-start-the-full-stack","title":"1. Start the Full Stack","text":"<pre><code># Start all services with migrations\ndocker-compose up -d\n\n# Check migration status\ndocker-compose logs migrations\n</code></pre>"},{"location":"MIGRATION_SETUP/#2-use-the-cli","title":"2. Use the CLI","text":"<pre><code># Start CLI service\ndocker-compose up cli\n\n# Or run a one-off command\ndocker-compose run --rm cli blacklake-cli repos list\n</code></pre>"},{"location":"MIGRATION_SETUP/#3-verify-setup","title":"3. Verify Setup","text":"<pre><code># Check all services are running\ndocker-compose ps\n\n# Check database schema\ndocker-compose exec db psql -U blacklake -d blacklake -c \"\\dt\"\n</code></pre>"},{"location":"MIGRATION_SETUP/#configuration","title":"\ud83d\udd27 Configuration","text":""},{"location":"MIGRATION_SETUP/#environment-variables","title":"Environment Variables","text":"<p>The migration and CLI services use these environment variables:</p> <pre><code># Database\nDATABASE_URL=postgresql://blacklake:blacklake@db:5432/blacklake\n\n# S3/MinIO\nS3_ENDPOINT=http://minio:9000\nS3_ACCESS_KEY_ID=minioadmin\nS3_SECRET_ACCESS_KEY=minioadmin\nS3_BUCKET=blacklake\nS3_REGION=us-east-1\n\n# API\nAPI_BASE_URL=http://api:8080\n\n# Logging\nRUST_LOG=info\n</code></pre>"},{"location":"MIGRATION_SETUP/#custom-migration-scripts","title":"Custom Migration Scripts","text":"<p>To add custom migrations:</p> <ol> <li>Create a new SQL file in <code>migrations/</code> with the next number</li> <li>Update <code>scripts/run-migrations.sh</code> to include your migration</li> <li>The migration will run automatically on next startup</li> </ol>"},{"location":"MIGRATION_SETUP/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"MIGRATION_SETUP/#migration-issues","title":"Migration Issues","text":"<pre><code># Check migration logs\ndocker-compose logs migrations\n\n# Run migrations manually\ndocker-compose run --rm migrations\n\n# Check database connection\ndocker-compose exec db psql -U blacklake -d blacklake -c \"SELECT version();\"\n</code></pre>"},{"location":"MIGRATION_SETUP/#cli-issues","title":"CLI Issues","text":"<pre><code># Check CLI logs\ndocker-compose logs cli\n\n# Test CLI connection\ndocker-compose run --rm cli blacklake-cli --help\n\n# Check environment variables\ndocker-compose run --rm cli env | grep -E \"(DATABASE_URL|S3_|API_)\"\n</code></pre>"},{"location":"MIGRATION_SETUP/#database-schema-issues","title":"Database Schema Issues","text":"<pre><code># Check current schema\ndocker-compose exec db psql -U blacklake -d blacklake -c \"\\dt\"\n\n# Reset database (WARNING: This will delete all data)\ndocker-compose down -v\ndocker-compose up -d db\ndocker-compose run --rm migrations\n</code></pre>"},{"location":"MIGRATION_SETUP/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Docker Compose Documentation</li> <li>SQLx Migration Guide</li> <li>BlackLake CLI Documentation</li> </ul>"},{"location":"OPERATIONS/","title":"BlackLake Operations Guide","text":"<p>This document provides comprehensive operations procedures for the BlackLake data platform.</p>"},{"location":"OPERATIONS/#operations-procedures","title":"Operations Procedures","text":""},{"location":"OPERATIONS/#health-monitoring","title":"Health Monitoring","text":"<pre><code># Check all services\ncurl http://localhost:8080/health\n\n# Check database\ncurl http://localhost:8080/health/db\n\n# Check storage\ncurl http://localhost:8080/health/storage\n</code></pre>"},{"location":"OPERATIONS/#backup-and-recovery","title":"Backup and Recovery","text":"<pre><code># Backup database\ndocker-compose exec db pg_dump -U blacklake blacklake &gt; backup.sql\n\n# Restore database\ndocker-compose exec -T db psql -U blacklake blacklake &lt; backup.sql\n</code></pre>"},{"location":"OPERATIONS/#additional-resources","title":"Additional Resources","text":"<ul> <li>Local Testing Guide</li> <li>Fast Setup Guide</li> <li>Migration Setup</li> <li>Project Status</li> <li>Implementation Summary</li> </ul>"},{"location":"OPERATIONS/#support","title":"Support","text":"<ul> <li>Documentation: Documentation Home</li> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> </ul>"},{"location":"PROJECT_STATUS/","title":"BlackLake Project Status","text":""},{"location":"PROJECT_STATUS/#current-status-production-ready","title":"Current Status: Production Ready \u2705","text":"<p>The BlackLake data platform has been fully implemented and is production-ready with comprehensive features across all major components.</p>"},{"location":"PROJECT_STATUS/#implementation-summary","title":"Implementation Summary","text":""},{"location":"PROJECT_STATUS/#completed-features-150-features-implemented","title":"\u2705 Completed Features (150+ features implemented)","text":""},{"location":"PROJECT_STATUS/#week-1-critical-infrastructure","title":"Week 1 - Critical Infrastructure","text":"<ul> <li>Authentication &amp; Security (JWT, OIDC, Rate Limiting, Circuit Breakers)</li> <li>Job Processing System (Apalis, Redis, ClamAV, CSV/Parquet sampling, ONNX sniffing, RDF generation)</li> <li>Solr Integration (Document indexing, search, suggestions, reindexing)</li> </ul>"},{"location":"PROJECT_STATUS/#week-2-core-api-features","title":"Week 2 - Core API Features","text":"<ul> <li>Connector Operations (Cloning, testing, syncing, status monitoring)</li> <li>Compliance Features (Retention policies, legal holds, audit logs, admin checks)</li> <li>Storage Operations (S3 configuration, retry logic, lifecycle policies, encryption)</li> <li>Governance &amp; Webhooks (Delivery tracking, database queries, retry scheduling)</li> </ul>"},{"location":"PROJECT_STATUS/#week-3-ui-implementation","title":"Week 3 - UI Implementation","text":"<ul> <li>Mobile Search API (Real API calls, suggestions, compliance, connectors)</li> <li>Mobile UI Components (Search context, store, pages, semantic search)</li> <li>Mobile UI Features (Pagination, file viewer, download, sharing, favorites, notifications)</li> </ul>"},{"location":"PROJECT_STATUS/#week-4-infrastructure-operations","title":"Week 4 - Infrastructure Operations","text":"<ul> <li>Database Operations (Connection pooling, retry logic, health checks, circuit breakers)</li> <li>Session Management (Redis integration, session statistics, monitoring)</li> <li>Export Functionality (Real tarball creation, file verification, error handling)</li> <li>Compliance Jobs (CSV export, legal holds, comprehensive reporting)</li> </ul>"},{"location":"PROJECT_STATUS/#week-5-performance-optimization","title":"Week 5 - Performance Optimization","text":"<ul> <li>Redis Caching (Search results, metadata, statistics, TTL management)</li> <li>Database Optimization (Query optimization, indexing, dynamic filtering, pagination)</li> <li>Monitoring &amp; Metrics (System, API, database, cache metrics)</li> <li>Analytics &amp; Reporting (Usage, performance, security analytics)</li> <li>Performance Testing (k6 load testing, stress testing, benchmarking)</li> </ul>"},{"location":"PROJECT_STATUS/#final-implementation-phase-remaining-critical-stubs","title":"Final Implementation Phase - Remaining Critical Stubs","text":"<ul> <li>RDF Metadata Processing (JSON-LD, Turtle conversion, S3 storage)</li> <li>ClamAV Virus Scanning (Real-time scanning, S3 integration, quarantine)</li> <li>Export Package Creation (Artifact collection, tarball creation, S3 upload)</li> <li>Reindex Job Processing (Apalis integration, batch processing, error handling)</li> </ul>"},{"location":"PROJECT_STATUS/#infrastructure-operations","title":"Infrastructure &amp; Operations","text":"<ul> <li>Active-Standby &amp; Disaster Recovery (Database replication, failover, health endpoints)</li> <li>Cost &amp; Lifecycle Governance (Cost estimation, usage metering, budget alerts)</li> <li>Access Reviews &amp; Data Egress Controls (Access review system, signed URL constraints)</li> <li>Performance Baseline &amp; Load Testing (k6 testing, performance reporting)</li> <li>Documentation System (MkDocs, Mermaid diagrams, CI for docs)</li> <li>Security &amp; Authentication (CSRF protection, API key auth, request signing)</li> <li>Infrastructure &amp; Operations (Kubernetes, Helm, HPA, service mesh, blue-green deployment)</li> </ul>"},{"location":"PROJECT_STATUS/#technical-architecture","title":"Technical Architecture","text":""},{"location":"PROJECT_STATUS/#core-components","title":"Core Components","text":"<ul> <li>API: Axum HTTP server with REST endpoints</li> <li>Core: Domain types, schemas, and business logic</li> <li>Index: PostgreSQL database access layer</li> <li>Storage: S3-compatible storage with presigned URLs</li> <li>ModelX: ONNX/PyTorch metadata sniffers</li> <li>CLI: Developer command-line interface</li> </ul>"},{"location":"PROJECT_STATUS/#infrastructure","title":"Infrastructure","text":"<ul> <li>Database: PostgreSQL with connection pooling and health monitoring</li> <li>Storage: S3-compatible storage with lifecycle policies and encryption</li> <li>Search: Solr integration with document indexing and search</li> <li>Caching: Redis for search results and metadata caching</li> <li>Monitoring: Comprehensive metrics and alerting</li> <li>Security: JWT/OIDC authentication, CSRF protection, rate limiting</li> </ul>"},{"location":"PROJECT_STATUS/#deployment","title":"Deployment","text":"<ul> <li>Docker Compose: Complete development environment</li> <li>Kubernetes: Production deployment with Helm charts</li> <li>CI/CD: Automated testing and deployment</li> <li>Monitoring: Prometheus, Grafana, Jaeger integration</li> </ul>"},{"location":"PROJECT_STATUS/#performance-metrics","title":"Performance Metrics","text":""},{"location":"PROJECT_STATUS/#system-performance","title":"System Performance","text":"<ul> <li>Response Time: &lt; 100ms for API calls</li> <li>Throughput: &gt; 1000 requests/second</li> <li>Uptime: 99.9% availability target</li> <li>Error Rate: &lt; 1% error rate</li> </ul>"},{"location":"PROJECT_STATUS/#database-performance","title":"Database Performance","text":"<ul> <li>Query Optimization: Dynamic filtering and indexing</li> <li>Connection Pooling: Efficient database connections</li> <li>Health Monitoring: Real-time database health checks</li> <li>Retry Logic: Exponential backoff for resilience</li> </ul>"},{"location":"PROJECT_STATUS/#storage-performance","title":"Storage Performance","text":"<ul> <li>S3 Integration: Efficient object storage</li> <li>Presigned URLs: Secure file access</li> <li>Lifecycle Policies: Cost optimization</li> <li>Encryption: Server-side encryption</li> </ul>"},{"location":"PROJECT_STATUS/#security-features","title":"Security Features","text":""},{"location":"PROJECT_STATUS/#authentication-authorization","title":"Authentication &amp; Authorization","text":"<ul> <li>JWT/OIDC: Secure token-based authentication</li> <li>Role-Based Access: Admin and user roles</li> <li>API Key Authentication: Programmatic access</li> <li>Request Signing: Secure API requests</li> </ul>"},{"location":"PROJECT_STATUS/#data-protection","title":"Data Protection","text":"<ul> <li>Virus Scanning: ClamAV integration</li> <li>Encryption: Server-side encryption</li> <li>Audit Logging: Comprehensive audit trails</li> <li>Access Controls: Fine-grained permissions</li> </ul>"},{"location":"PROJECT_STATUS/#compliance","title":"Compliance","text":"<ul> <li>Data Retention: Automated retention policies</li> <li>Legal Holds: Legal compliance features</li> <li>Audit Reports: Compliance reporting</li> <li>Data Classification: Automated data classification</li> </ul>"},{"location":"PROJECT_STATUS/#testing-quality","title":"Testing &amp; Quality","text":""},{"location":"PROJECT_STATUS/#test-coverage","title":"Test Coverage","text":"<ul> <li>Unit Tests: Comprehensive unit test coverage</li> <li>Integration Tests: End-to-end testing</li> <li>Performance Tests: Load and stress testing</li> <li>Security Tests: Penetration testing</li> </ul>"},{"location":"PROJECT_STATUS/#quality-assurance","title":"Quality Assurance","text":"<ul> <li>Code Review: Peer review process</li> <li>Static Analysis: Automated code analysis</li> <li>Security Scanning: Vulnerability scanning</li> <li>Performance Monitoring: Real-time performance tracking</li> </ul>"},{"location":"PROJECT_STATUS/#deployment-status","title":"Deployment Status","text":""},{"location":"PROJECT_STATUS/#development-environment","title":"Development Environment","text":"<ul> <li>\u2705 Docker Compose: Fully configured</li> <li>\u2705 Database: PostgreSQL with migrations</li> <li>\u2705 Storage: MinIO S3-compatible storage</li> <li>\u2705 Authentication: Keycloak OIDC provider</li> <li>\u2705 Monitoring: Prometheus, Grafana, Jaeger</li> </ul>"},{"location":"PROJECT_STATUS/#production-readiness","title":"Production Readiness","text":"<ul> <li>\u2705 Kubernetes: Production deployment manifests</li> <li>\u2705 Helm Charts: Package management</li> <li>\u2705 Monitoring: Comprehensive observability</li> <li>\u2705 Security: Production security measures</li> <li>\u2705 Documentation: Complete documentation</li> </ul>"},{"location":"PROJECT_STATUS/#future-roadmap","title":"Future Roadmap","text":""},{"location":"PROJECT_STATUS/#planned-enhancements","title":"Planned Enhancements","text":"<ul> <li>AI/ML Integration: Advanced ML model support</li> <li>GraphQL API: Complex query capabilities</li> <li>WebSocket Support: Real-time updates</li> <li>Advanced Analytics: Machine learning insights</li> </ul>"},{"location":"PROJECT_STATUS/#scalability-improvements","title":"Scalability Improvements","text":"<ul> <li>Microservices: Service decomposition</li> <li>Event Streaming: Apache Kafka integration</li> <li>Auto-scaling: Dynamic resource allocation</li> <li>Multi-region: Global deployment</li> </ul>"},{"location":"PROJECT_STATUS/#conclusion","title":"Conclusion","text":"<p>The BlackLake data platform is production-ready with comprehensive features, robust architecture, and extensive testing. All critical components have been implemented and tested, providing a solid foundation for data artifact management and ML operations.</p> <p>Status: \u2705 Production Ready Last Updated: 2024-01-15 Next Review: 2024-02-15</p>"},{"location":"QUICK_START/","title":"BlackLake Quick Start Guide","text":"<p>This guide will get you up and running with BlackLake using a simplified approach.</p>"},{"location":"QUICK_START/#step-by-step-setup","title":"\ud83d\ude80 Step-by-Step Setup","text":""},{"location":"QUICK_START/#1-prerequisites","title":"1. Prerequisites","text":"<pre><code># Install required tools\ncargo install cargo-chef\nbrew install just\n</code></pre>"},{"location":"QUICK_START/#2-build-all-services","title":"2. Build All Services","text":"<pre><code># Build all Docker images first\njust bake\n</code></pre>"},{"location":"QUICK_START/#3-start-everything-simplified","title":"3. Start Everything (Simplified)","text":"<pre><code># Start all services at once (simplified approach)\njust setup-step-by-step\n</code></pre>"},{"location":"QUICK_START/#4-alternative-manual-step-by-step","title":"4. Alternative: Manual Step-by-Step","text":"<pre><code># Step 1: Start database and dependencies\ndocker compose -f docker-compose.simple.yml up -d db minio redis keycloak solr\n\n# Step 2: Run migrations\njust migrate\n\n# Step 3: Start API and UI\ndocker compose -f docker-compose.simple.yml up -d api ui\n</code></pre>"},{"location":"QUICK_START/#5-verify-everything-is-working","title":"5. Verify Everything is Working","text":"<pre><code># Check all services\ndocker compose -f docker-compose.simple.yml ps\n\n# Test API\ncurl http://localhost:8080/health\n\n# Test UI\nopen http://localhost:5173\n\n# Test MinIO\nopen http://localhost:9001\n</code></pre>"},{"location":"QUICK_START/#alternative-one-command-setup","title":"\ud83d\udd27 Alternative: One-Command Setup","text":"<p>If you want to start everything at once (after building):</p> <pre><code># Build first\njust bake\n\n# Start everything\njust dev\n\n# Run migrations\njust migrate\n</code></pre>"},{"location":"QUICK_START/#test-cli-functionality","title":"\ud83e\uddea Test CLI Functionality","text":"<pre><code># Test CLI commands\njust cli-shell\n# Inside container: blacklake --help\n# Inside container: blacklake init --help\n# Inside container: blacklake put --help\n</code></pre>"},{"location":"QUICK_START/#service-urls","title":"\ud83c\udf10 Service URLs","text":"<ul> <li>API: http://localhost:8080</li> <li>UI (Dev): http://localhost:5173</li> <li>MinIO: http://localhost:9001</li> <li>Keycloak: http://localhost:8081</li> <li>PostgreSQL: localhost:5432</li> </ul>"},{"location":"QUICK_START/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"QUICK_START/#if-ui-doesnt-start","title":"If UI doesn't start:","text":"<pre><code># Check if UI service is running\ndocker compose ps ui-dev\n\n# Check UI logs\ndocker compose logs ui-dev\n\n# Restart UI\ndocker compose restart ui-dev\n</code></pre>"},{"location":"QUICK_START/#if-api-doesnt-start","title":"If API doesn't start:","text":"<pre><code># Check API logs\ndocker compose logs api\n\n# Check if database is ready\ndocker compose logs db\n</code></pre>"},{"location":"QUICK_START/#if-migrations-fail","title":"If migrations fail:","text":"<pre><code># Check database is running\ndocker compose ps db\n\n# Run migrations manually\ndocker compose run --rm migrations\n</code></pre>"},{"location":"QUICK_START/#service-dependencies","title":"\ud83d\udccb Service Dependencies","text":"<p>The correct startup order is: 1. Database (PostgreSQL) 2. Dependencies (MinIO, Redis, Keycloak, Solr) 3. Migrations (Database schema) 4. API (Backend service) 5. UI (Frontend service)</p>"},{"location":"QUICK_START/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<p>Once everything is running: 1. Test the API: Visit http://localhost:8080/health 2. Test the UI: Visit http://localhost:5173 3. Test CLI: Run <code>just cli-shell</code> 4. Create your first repository: Use the CLI to init and upload data</p>"},{"location":"SEARCH/","title":"Search in BlackLake","text":"<p>BlackLake provides powerful search capabilities to help you discover and explore your data. This guide covers all aspects of search functionality.</p>"},{"location":"SEARCH/#overview","title":"Overview","text":"<p>BlackLake's search is built on Apache Solr and provides:</p> <ul> <li>Full-text Search: Search across file contents and metadata</li> <li>Faceted Search: Filter results by file type, size, date, etc.</li> <li>Semantic Search: AI-powered semantic understanding</li> <li>Suggestions: Auto-complete and search suggestions</li> <li>Saved Searches: Save and share search queries</li> <li>Real-time Indexing: Search results update immediately</li> </ul>"},{"location":"SEARCH/#basic-search","title":"Basic Search","text":""},{"location":"SEARCH/#simple-text-search","title":"Simple Text Search","text":"<p>The most basic search is a simple text query:</p> <pre><code># CLI\nblacklake search \"machine learning\"\n\n# API\nGET /api/v1/search?q=machine%20learning\n</code></pre>"},{"location":"SEARCH/#search-operators","title":"Search Operators","text":"<p>BlackLake supports advanced search operators:</p>"},{"location":"SEARCH/#boolean-operators","title":"Boolean Operators","text":"<pre><code># AND search (both terms must be present)\nblacklake search \"machine AND learning\"\n\n# OR search (either term can be present)\nblacklake search \"machine OR learning\"\n\n# NOT search (exclude terms)\nblacklake search \"machine NOT python\"\n</code></pre>"},{"location":"SEARCH/#phrase-search","title":"Phrase Search","text":"<pre><code># Exact phrase matching\nblacklake search '\"machine learning\"'\n\n# Proximity search (terms within N words)\nblacklake search '\"machine learning\"~5'\n</code></pre>"},{"location":"SEARCH/#wildcard-search","title":"Wildcard Search","text":"<pre><code># Single character wildcard\nblacklake search \"mod?l\"\n\n# Multiple character wildcard\nblacklake search \"model*\"\n</code></pre>"},{"location":"SEARCH/#fuzzy-search","title":"Fuzzy Search","text":"<pre><code># Fuzzy matching (handles typos)\nblacklake search \"machne~\"  # matches \"machine\"\n</code></pre>"},{"location":"SEARCH/#advanced-search","title":"Advanced Search","text":""},{"location":"SEARCH/#field-specific-search","title":"Field-Specific Search","text":"<p>Search within specific metadata fields:</p> <pre><code># Search in file name\nblacklake search \"name:model.pkl\"\n\n# Search in file type\nblacklake search \"type:csv\"\n\n# Search in description\nblacklake search \"description:training data\"\n</code></pre>"},{"location":"SEARCH/#range-queries","title":"Range Queries","text":"<p>Search within ranges:</p> <pre><code># File size range\nblacklake search \"size:[1000000 TO 5000000]\"\n\n# Date range\nblacklake search \"created:[2024-01-01 TO 2024-12-31]\"\n\n# Numeric range\nblacklake search \"version:[1.0 TO 2.0]\"\n</code></pre>"},{"location":"SEARCH/#complex-queries","title":"Complex Queries","text":"<p>Combine multiple conditions:</p> <pre><code># Complex boolean query\nblacklake search \"(machine learning OR deep learning) AND (python OR tensorflow) AND size:[1000000 TO *]\"\n\n# Field-specific with ranges\nblacklake search \"type:csv AND size:[1000000 TO *] AND created:[2024-01-01 TO *]\"\n</code></pre>"},{"location":"SEARCH/#faceted-search","title":"Faceted Search","text":"<p>Faceted search allows you to filter results by categories:</p>"},{"location":"SEARCH/#available-facets","title":"Available Facets","text":"<ul> <li>File Type: Filter by MIME type (csv, json, pkl, etc.)</li> <li>Size: Filter by file size ranges</li> <li>Date: Filter by creation/modification date</li> <li>Repository: Filter by repository</li> <li>Tags: Filter by user-defined tags</li> <li>Owner: Filter by file owner</li> </ul>"},{"location":"SEARCH/#using-facets","title":"Using Facets","text":""},{"location":"SEARCH/#cli","title":"CLI","text":"<pre><code># Filter by file type\nblacklake search \"data\" --type csv\n\n# Filter by size\nblacklake search \"model\" --size \"&gt;1MB\"\n\n# Filter by date\nblacklake search \"dataset\" --created \"&gt;2024-01-01\"\n\n# Multiple filters\nblacklake search \"training\" --type csv --size \"&gt;1MB\" --created \"&gt;2024-01-01\"\n</code></pre>"},{"location":"SEARCH/#api","title":"API","text":"<pre><code># Faceted search via API\nGET /api/v1/search?q=data&amp;facet=type:csv&amp;facet=size:&gt;1MB&amp;facet=created:&gt;2024-01-01\n</code></pre>"},{"location":"SEARCH/#web-ui","title":"Web UI","text":"<ol> <li>Enter your search query</li> <li>Use the facet filters on the left sidebar</li> <li>Click on facet values to apply filters</li> <li>Use the \"Clear Filters\" button to reset</li> </ol>"},{"location":"SEARCH/#facet-statistics","title":"Facet Statistics","text":"<p>Get statistics about facet values:</p> <pre><code># Get file type distribution\nblacklake search \"data\" --facets type\n\n# Get size distribution\nblacklake search \"model\" --facets size\n\n# Get date distribution\nblacklake search \"dataset\" --facets created\n</code></pre>"},{"location":"SEARCH/#semantic-search","title":"Semantic Search","text":"<p>BlackLake includes AI-powered semantic search capabilities:</p>"},{"location":"SEARCH/#how-it-works","title":"How It Works","text":"<ol> <li>Embeddings: Files are processed to generate vector embeddings</li> <li>Vector Search: Queries are converted to embeddings and matched against file embeddings</li> <li>Ranking: Results are ranked by semantic similarity</li> <li>Hybrid Search: Combines traditional text search with semantic search</li> </ol>"},{"location":"SEARCH/#using-semantic-search","title":"Using Semantic Search","text":""},{"location":"SEARCH/#cli_1","title":"CLI","text":"<pre><code># Semantic search\nblacklake search \"find files related to image classification\" --semantic\n\n# Hybrid search (combines text and semantic)\nblacklake search \"machine learning models\" --hybrid\n</code></pre>"},{"location":"SEARCH/#api_1","title":"API","text":"<pre><code># Semantic search via API\nGET /api/v1/search?q=image%20classification&amp;semantic=true\n\n# Hybrid search\nGET /api/v1/search?q=machine%20learning&amp;hybrid=true\n</code></pre>"},{"location":"SEARCH/#web-ui_1","title":"Web UI","text":"<ol> <li>Enable \"Semantic Search\" in the search options</li> <li>Enter your query in natural language</li> <li>Results will be ranked by semantic similarity</li> </ol>"},{"location":"SEARCH/#semantic-search-examples","title":"Semantic Search Examples","text":"<pre><code># Find files related to a concept\nblacklake search \"files about data preprocessing\" --semantic\n\n# Find similar files\nblacklake search \"files similar to model.pkl\" --semantic\n\n# Find files by use case\nblacklake search \"files for training neural networks\" --semantic\n</code></pre>"},{"location":"SEARCH/#search-suggestions","title":"Search Suggestions","text":"<p>BlackLake provides intelligent search suggestions:</p>"},{"location":"SEARCH/#auto-complete","title":"Auto-complete","text":"<p>As you type, BlackLake suggests:</p> <ul> <li>File names: Matching file names</li> <li>Tags: User-defined tags</li> <li>Queries: Previously used search queries</li> <li>Concepts: AI-suggested concepts</li> </ul>"},{"location":"SEARCH/#using-suggestions","title":"Using Suggestions","text":""},{"location":"SEARCH/#cli_2","title":"CLI","text":"<pre><code># Get suggestions for a partial query\nblacklake search --suggest \"mach\"\n\n# Get suggestions for a specific field\nblacklake search --suggest \"name:mach\"\n</code></pre>"},{"location":"SEARCH/#api_2","title":"API","text":"<pre><code># Get suggestions via API\nGET /api/v1/search/suggest?q=mach\n\n# Get field-specific suggestions\nGET /api/v1/search/suggest?q=mach&amp;field=name\n</code></pre>"},{"location":"SEARCH/#web-ui_2","title":"Web UI","text":"<ul> <li>Suggestions appear as you type in the search box</li> <li>Click on suggestions to complete your query</li> <li>Use arrow keys to navigate suggestions</li> </ul>"},{"location":"SEARCH/#saved-searches","title":"Saved Searches","text":"<p>Save frequently used searches for quick access:</p>"},{"location":"SEARCH/#creating-saved-searches","title":"Creating Saved Searches","text":""},{"location":"SEARCH/#cli_3","title":"CLI","text":"<pre><code># Save a search\nblacklake search \"machine learning\" --save \"ml-files\"\n\n# Save with description\nblacklake search \"type:csv AND size:&gt;1MB\" --save \"large-csv-files\" --description \"Large CSV files\"\n</code></pre>"},{"location":"SEARCH/#web-ui_3","title":"Web UI","text":"<ol> <li>Perform your search</li> <li>Click \"Save Search\" button</li> <li>Enter a name and description</li> <li>Choose visibility (private or shared)</li> </ol>"},{"location":"SEARCH/#using-saved-searches","title":"Using Saved Searches","text":""},{"location":"SEARCH/#cli_4","title":"CLI","text":"<pre><code># List saved searches\nblacklake search --list-saved\n\n# Use a saved search\nblacklake search --saved \"ml-files\"\n\n# Update a saved search\nblacklake search \"machine learning AND python\" --update \"ml-files\"\n</code></pre>"},{"location":"SEARCH/#web-ui_4","title":"Web UI","text":"<ol> <li>Click on \"Saved Searches\" in the sidebar</li> <li>Click on a saved search to execute it</li> <li>Edit or delete saved searches from the management interface</li> </ol>"},{"location":"SEARCH/#sharing-saved-searches","title":"Sharing Saved Searches","text":"<pre><code># Share a saved search\nblacklake search --share \"ml-files\" --with \"team-member@example.com\"\n\n# Make a saved search public\nblacklake search --make-public \"ml-files\"\n</code></pre>"},{"location":"SEARCH/#search-configuration","title":"Search Configuration","text":""},{"location":"SEARCH/#search-settings","title":"Search Settings","text":"<p>Configure search behavior:</p>"},{"location":"SEARCH/#cli_5","title":"CLI","text":"<pre><code># Set default search options\nblacklake config set search.default-type semantic\nblacklake config set search.results-per-page 50\nblacklake config set search.highlight true\n</code></pre>"},{"location":"SEARCH/#configuration-file","title":"Configuration File","text":"<pre><code>[search]\ndefault_type = \"semantic\"\nresults_per_page = 50\nhighlight = true\nfacets = [\"type\", \"size\", \"created\", \"tags\"]\nsuggestions = true\n</code></pre>"},{"location":"SEARCH/#search-indexing","title":"Search Indexing","text":""},{"location":"SEARCH/#index-management","title":"Index Management","text":"<pre><code># Reindex a repository\nblacklake search --reindex my-repo\n\n# Reindex all repositories\nblacklake search --reindex-all\n\n# Check index status\nblacklake search --status\n</code></pre>"},{"location":"SEARCH/#index-optimization","title":"Index Optimization","text":"<pre><code># Optimize search index\nblacklake search --optimize\n\n# Check index health\nblacklake search --health\n</code></pre>"},{"location":"SEARCH/#performance-optimization","title":"Performance Optimization","text":""},{"location":"SEARCH/#search-performance","title":"Search Performance","text":""},{"location":"SEARCH/#query-optimization","title":"Query Optimization","text":"<ul> <li>Use specific fields: Search specific fields instead of all fields</li> <li>Limit results: Use pagination to limit result sets</li> <li>Use filters: Apply filters to reduce search scope</li> <li>Cache queries: Enable query caching for repeated searches</li> </ul>"},{"location":"SEARCH/#index-optimization_1","title":"Index Optimization","text":"<ul> <li>Regular reindexing: Keep indexes up to date</li> <li>Index optimization: Periodically optimize indexes</li> <li>Field selection: Only index necessary fields</li> <li>Stop words: Configure appropriate stop words</li> </ul>"},{"location":"SEARCH/#monitoring-search-performance","title":"Monitoring Search Performance","text":""},{"location":"SEARCH/#metrics","title":"Metrics","text":"<ul> <li>Query time: Average query response time</li> <li>Index size: Search index size</li> <li>Cache hit rate: Query cache effectiveness</li> <li>Error rate: Search error rate</li> </ul>"},{"location":"SEARCH/#monitoring-commands","title":"Monitoring Commands","text":"<pre><code># Get search statistics\nblacklake search --stats\n\n# Monitor search performance\nblacklake search --monitor\n\n# Check search health\nblacklake search --health\n</code></pre>"},{"location":"SEARCH/#search-api-reference","title":"Search API Reference","text":""},{"location":"SEARCH/#endpoints","title":"Endpoints","text":""},{"location":"SEARCH/#basic-search_1","title":"Basic Search","text":"<pre><code>GET /api/v1/search?q={query}\n</code></pre>"},{"location":"SEARCH/#faceted-search_1","title":"Faceted Search","text":"<pre><code>GET /api/v1/search?q={query}&amp;facet={field}:{value}\n</code></pre>"},{"location":"SEARCH/#semantic-search_1","title":"Semantic Search","text":"<pre><code>GET /api/v1/search?q={query}&amp;semantic=true\n</code></pre>"},{"location":"SEARCH/#search-suggestions_1","title":"Search Suggestions","text":"<pre><code>GET /api/v1/search/suggest?q={query}\n</code></pre>"},{"location":"SEARCH/#saved-searches_1","title":"Saved Searches","text":"<pre><code>GET /api/v1/search/saved\nPOST /api/v1/search/saved\nPUT /api/v1/search/saved/{id}\nDELETE /api/v1/search/saved/{id}\n</code></pre>"},{"location":"SEARCH/#response-format","title":"Response Format","text":"<pre><code>{\n  \"query\": \"machine learning\",\n  \"total_results\": 150,\n  \"results\": [\n    {\n      \"id\": \"file-123\",\n      \"name\": \"model.pkl\",\n      \"path\": \"/models/model.pkl\",\n      \"type\": \"application/octet-stream\",\n      \"size\": 2048576,\n      \"created\": \"2024-01-15T10:30:00Z\",\n      \"modified\": \"2024-01-15T10:30:00Z\",\n      \"metadata\": {\n        \"description\": \"Machine learning model\",\n        \"tags\": [\"ml\", \"model\", \"python\"]\n      },\n      \"highlights\": {\n        \"content\": \"This is a &lt;em&gt;machine learning&lt;/em&gt; model\"\n      }\n    }\n  ],\n  \"facets\": {\n    \"type\": {\n      \"csv\": 45,\n      \"json\": 30,\n      \"pkl\": 25\n    },\n    \"size\": {\n      \"&lt;1MB\": 60,\n      \"1MB-10MB\": 70,\n      \"&gt;10MB\": 20\n    }\n  },\n  \"suggestions\": [\n    \"machine learning\",\n    \"machine learning models\",\n    \"machine learning algorithms\"\n  ]\n}\n</code></pre>"},{"location":"SEARCH/#best-practices","title":"Best Practices","text":""},{"location":"SEARCH/#search-query-design","title":"Search Query Design","text":"<ol> <li>Start Simple: Begin with basic text search</li> <li>Add Filters: Use facets to narrow results</li> <li>Use Operators: Leverage boolean operators for complex queries</li> <li>Test Queries: Validate search queries before saving</li> <li>Monitor Performance: Watch for slow queries</li> </ol>"},{"location":"SEARCH/#search-index-management","title":"Search Index Management","text":"<ol> <li>Regular Updates: Keep search indexes current</li> <li>Optimize Periodically: Run index optimization</li> <li>Monitor Size: Watch index size growth</li> <li>Backup Indexes: Include search indexes in backups</li> <li>Test Restores: Verify search index restoration</li> </ol>"},{"location":"SEARCH/#user-experience","title":"User Experience","text":"<ol> <li>Provide Suggestions: Enable auto-complete</li> <li>Show Facets: Display available filters</li> <li>Highlight Results: Highlight matching terms</li> <li>Save Searches: Allow saving frequent queries</li> <li>Share Results: Enable sharing of search results</li> </ol> <p>This comprehensive search functionality makes BlackLake a powerful platform for data discovery and exploration. The combination of traditional text search, faceted filtering, and semantic search provides users with multiple ways to find the data they need.</p>"},{"location":"SESSIONS/","title":"BlackLake Session Management","text":""},{"location":"SESSIONS/#overview","title":"Overview","text":"<p>BlackLake implements secure server-side sessions using <code>tower-sessions</code> with Redis as the backing store. This provides a seamless web experience while maintaining security best practices.</p>"},{"location":"SESSIONS/#architecture","title":"Architecture","text":""},{"location":"SESSIONS/#session-storage","title":"Session Storage","text":"<ul> <li>Backend: Redis with key namespace <code>bl:sess:</code></li> <li>Format: Encrypted JSON with HMAC-SHA256 signing</li> <li>TTL: 12 hours sliding window, 7 days maximum absolute</li> <li>Cookie: <code>blksess</code> with secure, httpOnly, sameSite=Lax</li> </ul>"},{"location":"SESSIONS/#authentication-flow","title":"Authentication Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Browser   \u2502    \u2502   BlackLake  \u2502    \u2502   Keycloak  \u2502\n\u2502             \u2502    \u2502     API      \u2502    \u2502             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                   \u2502                   \u2502\n       \u2502 1. Login Request  \u2502                   \u2502\n       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502                   \u2502\n       \u2502                   \u2502 2. OIDC Auth     \u2502\n       \u2502                   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502\n       \u2502                   \u2502 3. JWT Token     \u2502\n       \u2502                   \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n       \u2502 4. Session Cookie \u2502                   \u2502\n       \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                   \u2502\n       \u2502                   \u2502                   \u2502\n       \u2502 5. API Requests   \u2502                   \u2502\n       \u2502    (with cookie)  \u2502                   \u2502\n       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502                   \u2502\n       \u2502                   \u2502 6. Session Lookup\u2502\n       \u2502                   \u2502    (Redis)        \u2502\n       \u2502                   \u2502                   \u2502\n       \u2502 7. Response       \u2502                   \u2502\n       \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                   \u2502\n</code></pre>"},{"location":"SESSIONS/#session-data-structure","title":"Session Data Structure","text":""},{"location":"SESSIONS/#authsession","title":"AuthSession","text":"<pre><code>pub struct AuthSession {\n    pub sub: String,                    // User ID from OIDC\n    pub email: String,                  // User email\n    pub roles: Vec&lt;String&gt;,             // User roles/permissions\n    pub oidc_token_metadata: Option&lt;serde_json::Value&gt;, // Token claims\n    pub csrf_token: CSRFToken,          // CSRF protection token\n}\n</code></pre>"},{"location":"SESSIONS/#session-storage-format","title":"Session Storage Format","text":"<pre><code>{\n  \"auth_session\": {\n    \"sub\": \"user-123\",\n    \"email\": \"user@example.com\", \n    \"roles\": [\"user\", \"admin\"],\n    \"oidc_token_metadata\": {\n      \"iss\": \"http://keycloak:8080/realms/master\",\n      \"aud\": \"blacklake\",\n      \"exp\": 1640995200\n    },\n    \"csrf_token\": \"base64-encoded-random-token\"\n  }\n}\n</code></pre>"},{"location":"SESSIONS/#security-features","title":"Security Features","text":""},{"location":"SESSIONS/#csrf-protection","title":"CSRF Protection","text":"<ul> <li>Double Submit Pattern: CSRF token in both cookie and header</li> <li>Header Name: <code>x-csrf-token</code></li> <li>Validation: Required for all state-changing requests (POST, PUT, DELETE)</li> <li>Exemption: GET and HEAD requests bypass CSRF check</li> </ul>"},{"location":"SESSIONS/#session-security","title":"Session Security","text":"<ul> <li>Encryption: Session data encrypted with 256-bit key</li> <li>Signing: HMAC-SHA256 prevents tampering</li> <li>Secure Cookies: HTTPS-only in production</li> <li>HttpOnly: Prevents XSS access to session cookies</li> <li>SameSite: Lax policy prevents CSRF attacks</li> </ul>"},{"location":"SESSIONS/#key-management","title":"Key Management","text":"<ul> <li>Session Secret: 256-bit (32-byte) base64-encoded key</li> <li>Environment Variable: <code>SESSION_SECRET</code></li> <li>Rotation: Support for key rotation without session invalidation</li> <li>Generation: <code>openssl rand -base64 32</code> for new secrets</li> </ul>"},{"location":"SESSIONS/#api-endpoints","title":"API Endpoints","text":""},{"location":"SESSIONS/#session-management","title":"Session Management","text":"<pre><code># Create session (after OIDC login)\nPOST /v1/session/login\nContent-Type: application/json\n{\n  \"oidc_token\": \"jwt-token-here\"\n}\n\n# Get CSRF token\nGET /v1/csrf\nResponse: {\n  \"csrf_token\": \"base64-token\"\n}\n\n# Logout (revoke session)\nPOST /v1/session/logout\nx-csrf-token: base64-token\n</code></pre>"},{"location":"SESSIONS/#authentication-middleware","title":"Authentication Middleware","text":"<pre><code>// Extract AuthContext from session\n#[async_trait]\nimpl&lt;S&gt; FromRequestParts&lt;S&gt; for AuthContext\nwhere\n    S: Send + Sync,\n    RedisStore: FromRef&lt;S&gt;,\n{\n    type Rejection = SessionError;\n\n    async fn from_request_parts(parts: &amp;mut Parts, state: &amp;S) -&gt; Result&lt;Self, Self::Rejection&gt; {\n        // 1. Extract session from cookie\n        // 2. Validate CSRF token for state-changing requests\n        // 3. Return AuthContext with user info\n    }\n}\n</code></pre>"},{"location":"SESSIONS/#configuration","title":"Configuration","text":""},{"location":"SESSIONS/#environment-variables","title":"Environment Variables","text":"<pre><code># Required\nSESSION_SECRET=base64-encoded-32-byte-key\nREDIS_URL=redis://redis:6379\n\n# Optional\nSESSION_TTL_HOURS=12\nSESSION_MAX_AGE_DAYS=7\nSESSION_COOKIE_NAME=blksess\n</code></pre>"},{"location":"SESSIONS/#session-layer-configuration","title":"Session Layer Configuration","text":"<pre><code>let session_layer = SessionManagerLayer::new(store)\n    .with_cookie_name(\"blksess\")\n    .with_expiry(Expiry::OnInactivity(CookieDuration::hours(12)))\n    .with_absolute_expiry(Expiry::OnApplicationClosure(CookieDuration::days(7)))\n    .with_secure(true)                    // HTTPS only\n    .with_http_only(true)                 // No JS access\n    .with_same_site(SameSite::Lax)        // CSRF protection\n    .with_signed(secret.as_bytes());      // HMAC signing\n</code></pre>"},{"location":"SESSIONS/#usage-examples","title":"Usage Examples","text":""},{"location":"SESSIONS/#react-frontend","title":"React Frontend","text":"<pre><code>// Login flow\nconst login = async (oidcToken: string) =&gt; {\n  const response = await fetch('/v1/session/login', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ oidc_token: oidcToken })\n  });\n\n  if (response.ok) {\n    // Session cookie automatically set\n    // Redirect to dashboard\n  }\n};\n\n// API requests with CSRF protection\nconst apiRequest = async (url: string, options: RequestInit = {}) =&gt; {\n  // Get CSRF token\n  const csrfResponse = await fetch('/v1/csrf');\n  const { csrf_token } = await csrfResponse.json();\n\n  return fetch(url, {\n    ...options,\n    headers: {\n      ...options.headers,\n      'x-csrf-token': csrf_token\n    }\n  });\n};\n\n// Logout\nconst logout = async () =&gt; {\n  await apiRequest('/v1/session/logout', { method: 'POST' });\n  // Redirect to login page\n};\n</code></pre>"},{"location":"SESSIONS/#cliapi-authentication","title":"CLI/API Authentication","text":"<pre><code># CLI continues to use OIDC bearer tokens\nexport BLACKLAKE_TOKEN=\"eyJhbGciOiJSUzI1NiIs...\"\n\n# API requests with bearer token\ncurl -H \"Authorization: Bearer $BLACKLAKE_TOKEN\" \\\n     http://localhost:8080/v1/repos\n</code></pre>"},{"location":"SESSIONS/#monitoring-observability","title":"Monitoring &amp; Observability","text":""},{"location":"SESSIONS/#metrics","title":"Metrics","text":"<ul> <li>Session Count: Active sessions in Redis</li> <li>Session Duration: Average and P95 session lifetime</li> <li>CSRF Failures: Failed CSRF token validations</li> <li>Session Expiry: Rate of session timeouts</li> <li>Login Rate: Successful vs failed logins</li> </ul>"},{"location":"SESSIONS/#health-checks","title":"Health Checks","text":"<pre><code># Check Redis connectivity\nredis-cli -u $REDIS_URL ping\n\n# Check session store\ncurl http://localhost:8080/v1/health/sessions\n\n# Monitor session metrics\ncurl http://localhost:8080/metrics | grep session\n</code></pre>"},{"location":"SESSIONS/#troubleshooting","title":"Troubleshooting","text":""},{"location":"SESSIONS/#common-issues","title":"Common Issues","text":"<ol> <li>Session Not Persisting: Check Redis connectivity and SESSION_SECRET</li> <li>CSRF Failures: Verify token is sent in correct header</li> <li>Cookie Not Set: Check secure flag and domain settings</li> <li>Session Expiry: Verify TTL configuration and Redis persistence</li> </ol>"},{"location":"SESSIONS/#debug-commands","title":"Debug Commands","text":"<pre><code># Check Redis session keys\nredis-cli -u $REDIS_URL keys \"bl:sess:*\"\n\n# Inspect session data\nredis-cli -u $REDIS_URL get \"bl:sess:session-id\"\n\n# Clear all sessions (emergency)\nredis-cli -u $REDIS_URL flushdb\n</code></pre>"},{"location":"SESSIONS/#migration-from-jwt-only","title":"Migration from JWT-Only","text":""},{"location":"SESSIONS/#backward-compatibility","title":"Backward Compatibility","text":"<ul> <li>API Endpoints: Continue to accept OIDC bearer tokens</li> <li>CLI Tools: No changes required</li> <li>Web UI: Migrate to session-based authentication</li> <li>Gradual Rollout: Feature flag for session vs JWT auth</li> </ul>"},{"location":"SESSIONS/#migration-steps","title":"Migration Steps","text":"<ol> <li>Deploy Session Support: Add session layer to API</li> <li>Update Frontend: Implement session-based auth flow</li> <li>Test Compatibility: Verify both auth methods work</li> <li>Monitor Metrics: Track session adoption and performance</li> <li>Deprecate JWT: Eventually remove JWT support for web UI</li> </ol>"},{"location":"SESSIONS/#security-considerations","title":"Security Considerations","text":""},{"location":"SESSIONS/#best-practices","title":"Best Practices","text":"<ul> <li>Regular Key Rotation: Rotate SESSION_SECRET periodically</li> <li>Session Monitoring: Alert on unusual session patterns</li> <li>Rate Limiting: Prevent session creation abuse</li> <li>Audit Logging: Log all session events for security analysis</li> </ul>"},{"location":"SESSIONS/#threat-mitigation","title":"Threat Mitigation","text":"<ul> <li>Session Hijacking: Secure cookies and HTTPS prevent interception</li> <li>CSRF Attacks: Double submit tokens prevent cross-site requests</li> <li>Session Fixation: Generate new session ID on login</li> <li>Brute Force: Rate limiting and account lockout policies</li> </ul>"},{"location":"TODO/","title":"BlackLake TODO - Future Work &amp; Enhancements","text":"<p>This document tracks future enhancement ideas and potential improvements for the BlackLake data platform.</p> <p>Note: All critical implementation stubs have been completed and moved to CHANGELOG.md. This document now focuses on future enhancements and potential improvements.</p>"},{"location":"TODO/#future-enhancement-ideas","title":"\ud83d\ude80 Future Enhancement Ideas","text":""},{"location":"TODO/#advanced-features","title":"Advanced Features","text":"<ul> <li> AI &amp; ML Integration</li> <li> Full-text search capabilities with AI-powered ranking</li> <li> Semantic search for ML models with vector embeddings</li> <li> Data lineage visualization with graph algorithms</li> <li> GraphQL API for complex queries and relationships</li> <li> WebSocket support for real-time updates and notifications</li> </ul>"},{"location":"TODO/#operations-infrastructure","title":"Operations &amp; Infrastructure","text":"<ul> <li> Cloud-Native Enhancements</li> <li> Cross-region replication for disaster recovery</li> <li> Read replicas for better performance and scaling</li> <li> Advanced Kubernetes operators for automated management</li> <li> Service mesh integration (Istio/Linkerd) for traffic management</li> <li> GitOps deployment with ArgoCD</li> </ul>"},{"location":"TODO/#integrations-ecosystem","title":"Integrations &amp; Ecosystem","text":"<ul> <li> ML/AI Platform Integration</li> <li> MLflow integration for experiment tracking and model management</li> <li> Weights &amp; Biases integration for experiment visualization</li> <li> Kubeflow pipeline integration for ML workflows</li> <li> DVC (Data Version Control) compatibility for data versioning</li> <li> <p> Jupyter notebook integration for interactive data science</p> </li> <li> <p> Cloud Provider Integration</p> </li> <li> AWS S3, Azure Blob, GCP Cloud Storage support</li> <li> Cloud-native authentication (IAM, RBAC)</li> <li> Cloud monitoring and logging integration (CloudWatch, Azure Monitor, GCP Monitoring)</li> <li> Cloud cost optimization and billing integration</li> </ul>"},{"location":"TODO/#compliance-governance","title":"Compliance &amp; Governance","text":"<ul> <li> Advanced Compliance</li> <li> GDPR compliance features with data subject rights</li> <li> Data classification and handling with automated policies</li> <li> Privacy-preserving features with differential privacy</li> <li> Advanced audit trail and compliance reporting</li> <li> Data residency and sovereignty controls</li> </ul>"},{"location":"TODO/#data-management-schema","title":"Data Management &amp; Schema","text":"<ul> <li> Advanced Data Features</li> <li> Data lineage visualization with interactive graphs</li> <li> Data retention policies with automated enforcement</li> <li> Search result ranking and relevance with ML</li> <li> Search analytics and optimization with A/B testing</li> <li> Data quality monitoring and validation</li> </ul>"},{"location":"TODO/#repository-version-control","title":"Repository &amp; Version Control","text":"<ul> <li> Advanced Repository Features</li> <li> Repository name collision detection with retry logic</li> <li> Repository size limits and quotas with enforcement</li> <li> Repository archiving and deletion with lifecycle management</li> <li> Repository templates and initialization with best practices</li> <li> <p> Repository forking and branching strategies with Git-like workflows</p> </li> <li> <p> Advanced Commit Features</p> </li> <li> Commit message validation and sanitization</li> <li> Atomic commit operations with proper rollback</li> <li> Commit size limits and validation</li> <li> Branch protection rules and merge policies</li> <li> Commit signing and verification with GPG</li> <li> Cherry-picking and rebasing operations</li> </ul>"},{"location":"TODO/#access-control-security","title":"Access Control &amp; Security","text":"<ul> <li> Advanced Access Control</li> <li> Fine-grained permissions (read/write/admin) with resource-level controls</li> <li> Team-based access control with group management</li> <li> Repository-level and organization-level permissions</li> <li> Audit logging for all access operations</li> <li> Permission inheritance and delegation</li> </ul>"},{"location":"TODO/#model-management-ml","title":"Model Management &amp; ML","text":"<ul> <li> Advanced Model Support</li> <li> TensorFlow SavedModel support with metadata extraction</li> <li> Hugging Face model format support with tokenizer integration</li> <li> Scikit-learn model serialization with joblib support</li> <li> Custom model format plugins with extensible architecture</li> <li> <p> Model format validation and conversion</p> </li> <li> <p> Model Lifecycle Management</p> </li> <li> Model versioning and tagging with semantic versioning</li> <li> Model deployment tracking with environment management</li> <li> Model performance monitoring with drift detection</li> <li> Model rollback and rollforward capabilities</li> <li> Model deprecation and sunset policies</li> </ul>"},{"location":"TODO/#performance-scalability","title":"Performance &amp; Scalability","text":"<ul> <li> Advanced Performance Features</li> <li> CDN integration for blob downloads with edge caching</li> <li> Database partitioning for large tables with sharding</li> <li> Load balancing configuration with health checks</li> <li> Database sharding strategies with consistent hashing</li> <li> <p> Microservices architecture planning with domain boundaries</p> </li> <li> <p> Advanced Architecture Patterns</p> </li> <li> Event sourcing for audit trails and state reconstruction</li> <li> CQRS (Command Query Responsibility Segregation) for read/write separation</li> <li> Event streaming with Apache Kafka or AWS Kinesis</li> <li> Auto-scaling based on metrics with predictive scaling</li> <li> Cross-region replication with eventual consistency</li> </ul>"},{"location":"TODO/#reliability-operations","title":"Reliability &amp; Operations","text":"<ul> <li> Advanced Reliability Features</li> <li> Point-in-time recovery with continuous backup</li> <li> Disaster recovery testing with chaos engineering</li> <li> Business continuity planning with RTO/RPO targets</li> <li> GraphQL API for complex queries with schema stitching</li> <li> WebSocket support for real-time updates with connection management</li> </ul>"},{"location":"TODO/#developer-experience","title":"Developer Experience","text":"<ul> <li> Advanced Developer Features</li> <li> API versioning and backward compatibility with semantic versioning</li> <li> API documentation with OpenAPI/Swagger and interactive examples</li> <li> API client SDKs (Python, JavaScript, Go, Java, C#)</li> <li> Interactive mode and shell integration with REPL</li> <li> <p> Progress bars for long operations with cancellation</p> </li> <li> <p> Advanced CLI Features</p> </li> <li> Configuration file support with environment-specific configs</li> <li> Plugin system for custom commands with extensibility</li> <li> Tab completion and help system with context-aware suggestions</li> <li> Chaos engineering and fault injection with controlled failures</li> <li> Security testing and penetration testing with automated scans</li> </ul>"},{"location":"TODO/#testing-quality","title":"Testing &amp; Quality","text":"<ul> <li> Advanced Testing Features</li> <li> Performance regression detection with baseline comparison</li> <li> Contract testing for API compatibility</li> <li> Mutation testing for test quality assessment</li> <li> Deployment and operations runbooks with step-by-step procedures</li> <li> User guides and tutorials with interactive examples</li> </ul>"},{"location":"TODO/#documentation-support","title":"Documentation &amp; Support","text":"<ul> <li> Advanced Documentation Features</li> <li> Architecture decision records (ADRs) with rationale</li> <li> Troubleshooting guides with common issues and solutions</li> <li> Developer onboarding documentation with environment setup</li> <li> Video tutorials and demos with screen recordings</li> <li> <p> Community support channels with forums and chat</p> </li> <li> <p> Advanced Support Features</p> </li> <li> FAQ and knowledge base with search functionality</li> <li> Training materials for operations team with hands-on labs</li> <li> OCI (Open Container Initiative) standards compliance</li> <li> MLflow model format compatibility with version support</li> <li> ONNX model standard support with optimization</li> </ul>"},{"location":"TODO/#standards-compliance","title":"Standards &amp; Compliance","text":"<ul> <li> Advanced Standards Support</li> <li> MLOps best practices compliance with automation</li> <li> Data governance standards with policy enforcement</li> <li> Security standards compliance with automated checks</li> <li> Performance standards with monitoring and alerting</li> <li> Quality standards with automated testing and validation</li> </ul>"},{"location":"TODO/#implementation-priority","title":"\ud83d\udccb Implementation Priority","text":""},{"location":"TODO/#high-priority-p0","title":"High Priority (P0)","text":"<ul> <li>Critical security vulnerabilities</li> <li>Performance bottlenecks</li> <li>Data loss prevention</li> <li>System stability issues</li> </ul>"},{"location":"TODO/#medium-priority-p1","title":"Medium Priority (P1)","text":"<ul> <li>User experience improvements</li> <li>Performance optimizations</li> <li>Feature enhancements</li> <li>Integration improvements</li> </ul>"},{"location":"TODO/#low-priority-p2","title":"Low Priority (P2)","text":"<ul> <li>Nice-to-have features</li> <li>Future technology adoption</li> <li>Advanced analytics</li> <li>Experimental features</li> </ul>"},{"location":"TODO/#success-metrics","title":"\ud83c\udfaf Success Metrics","text":""},{"location":"TODO/#technical-metrics","title":"Technical Metrics","text":"<ul> <li>Performance: Response time &lt; 100ms, throughput &gt; 1000 req/s</li> <li>Reliability: 99.9% uptime, &lt; 1% error rate</li> <li>Security: Zero critical vulnerabilities, 100% compliance</li> <li>Quality: 90%+ test coverage, &lt; 5% technical debt</li> </ul>"},{"location":"TODO/#business-metrics","title":"Business Metrics","text":"<ul> <li>User Adoption: 80%+ user satisfaction, 50% growth</li> <li>Operational Efficiency: 30% reduction in manual tasks</li> <li>Cost Optimization: 20% reduction in infrastructure costs</li> <li>Time to Market: 50% faster feature delivery</li> </ul>"},{"location":"TODO/#review-process","title":"\ud83d\udd04 Review Process","text":""},{"location":"TODO/#monthly-reviews","title":"Monthly Reviews","text":"<ul> <li>Review and update priority rankings</li> <li>Assess progress against success metrics</li> <li>Identify new enhancement opportunities</li> <li>Update implementation timelines</li> </ul>"},{"location":"TODO/#quarterly-reviews","title":"Quarterly Reviews","text":"<ul> <li>Comprehensive feature assessment</li> <li>Technology stack evaluation</li> <li>Performance and scalability analysis</li> <li>Strategic roadmap updates</li> </ul>"},{"location":"TODO/#annual-reviews","title":"Annual Reviews","text":"<ul> <li>Complete platform assessment</li> <li>Long-term strategic planning</li> <li>Technology roadmap updates</li> <li>Competitive analysis and positioning</li> </ul> <p>Last Updated: 2024-01-15 Next Review: 2024-02-15</p>"},{"location":"VERIFICATION/","title":"BlackLake Verification Guide","text":"<p>This document provides comprehensive verification procedures for all BlackLake systems and components.</p>"},{"location":"VERIFICATION/#system-verification-checklist","title":"System Verification Checklist","text":""},{"location":"VERIFICATION/#infrastructure-components","title":"\u2705 Infrastructure Components","text":""},{"location":"VERIFICATION/#database-postgresql","title":"Database (PostgreSQL)","text":"<ul> <li> Database connection established</li> <li> Migration scripts executed successfully</li> <li> Connection pooling configured</li> <li> Health checks responding</li> <li> Query performance optimized</li> <li> Backup procedures tested</li> </ul>"},{"location":"VERIFICATION/#storage-s3minio","title":"Storage (S3/MinIO)","text":"<ul> <li> S3 connection established</li> <li> Bucket creation and configuration</li> <li> Lifecycle policies applied</li> <li> Encryption enabled</li> <li> Presigned URL generation</li> <li> Health checks responding</li> </ul>"},{"location":"VERIFICATION/#search-solr","title":"Search (Solr)","text":"<ul> <li> Solr connection established</li> <li> Document indexing working</li> <li> Search queries responding</li> <li> Suggestions working</li> <li> Reindexing functional</li> <li> Health checks responding</li> </ul>"},{"location":"VERIFICATION/#cache-redis","title":"Cache (Redis)","text":"<ul> <li> Redis connection established</li> <li> Cache operations working</li> <li> TTL configuration correct</li> <li> Statistics collection working</li> <li> Health checks responding</li> </ul>"},{"location":"VERIFICATION/#api-endpoints","title":"\u2705 API Endpoints","text":""},{"location":"VERIFICATION/#authentication","title":"Authentication","text":"<ul> <li> JWT token validation</li> <li> OIDC integration</li> <li> Rate limiting working</li> <li> Request timeouts configured</li> <li> Circuit breaker functional</li> </ul>"},{"location":"VERIFICATION/#repository-operations","title":"Repository Operations","text":"<ul> <li> Create repository</li> <li> List repositories</li> <li> Get repository details</li> <li> Update repository</li> <li> Delete repository</li> </ul>"},{"location":"VERIFICATION/#file-operations","title":"File Operations","text":"<ul> <li> Upload files</li> <li> Download files</li> <li> List files</li> <li> Delete files</li> <li> File metadata extraction</li> </ul>"},{"location":"VERIFICATION/#search-operations","title":"Search Operations","text":"<ul> <li> Text search</li> <li> Metadata search</li> <li> Filtered search</li> <li> Pagination</li> <li> Search suggestions</li> </ul>"},{"location":"VERIFICATION/#verification-procedures","title":"Verification Procedures","text":""},{"location":"VERIFICATION/#1-system-health-checks","title":"1. System Health Checks","text":"<pre><code># Check all services\ncurl http://localhost:8080/health\n\n# Check database\ncurl http://localhost:8080/health/db\n\n# Check storage\ncurl http://localhost:8080/health/storage\n\n# Check search\ncurl http://localhost:8080/health/search\n\n# Check cache\ncurl http://localhost:8080/health/cache\n</code></pre>"},{"location":"VERIFICATION/#2-authentication-verification","title":"2. Authentication Verification","text":"<pre><code># Get JWT token\nTOKEN=$(curl -s -X POST http://localhost:8081/realms/blacklake/protocol/openid-connect/token \\\n  -H \"Content-Type: application/x-www-form-urlencoded\" \\\n  -d \"username=admin&amp;password=admin&amp;grant_type=password&amp;client_id=blacklake\" | jq -r .access_token)\n\n# Test authenticated request\ncurl -H \"Authorization: Bearer $TOKEN\" http://localhost:8080/api/v1/repos\n</code></pre>"},{"location":"VERIFICATION/#additional-resources","title":"Additional Resources","text":"<ul> <li>Local Testing Guide</li> <li>Fast Setup Guide</li> <li>Migration Setup</li> <li>Project Status</li> <li>Implementation Summary</li> </ul>"},{"location":"VERIFICATION/#support","title":"Support","text":"<ul> <li>Documentation: Documentation Home</li> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> </ul>"},{"location":"architecture/","title":"BlackLake Architecture","text":"<p>This document provides a comprehensive overview of the BlackLake architecture, including system components, data flow, and design principles.</p>"},{"location":"architecture/#system-overview","title":"System Overview","text":"<p>BlackLake is built as a modern, cloud-native data platform with the following key characteristics:</p> <ul> <li>Microservices Architecture: Loosely coupled services with clear boundaries</li> <li>API-First Design: RESTful APIs with OpenAPI specifications</li> <li>Event-Driven: Asynchronous processing with event sourcing</li> <li>Multi-tenant: Secure isolation between organizations</li> <li>Scalable: Horizontal scaling capabilities</li> <li>Observable: Comprehensive monitoring and logging</li> </ul>"},{"location":"architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TB\n    subgraph \"External Users\"\n        U1[Data Scientists]\n        U2[ML Engineers]\n        U3[Data Engineers]\n        U4[System Administrators]\n    end\n\n    subgraph \"BlackLake Platform\"\n        subgraph \"API Gateway\"\n            GW[Load Balancer]\n            AUTH[Authentication Service]\n            RATE[Rate Limiter]\n        end\n\n        subgraph \"Core Services\"\n            API[Rust API Server]\n            CLI[CLI Tool]\n            UI[React Web UI]\n            MOBILE[Mobile UI]\n        end\n\n        subgraph \"Data Layer\"\n            DB[(PostgreSQL)]\n            REDIS[(Redis Cache)]\n            SOLR[(Apache Solr)]\n        end\n\n        subgraph \"Storage Layer\"\n            S3[S3-Compatible Storage]\n            MINIO[MinIO]\n        end\n\n        subgraph \"Job Processing\"\n            JOB[Apalis Job Processor]\n            QUEUE[Redis Queue]\n            WORKER[Background Workers]\n        end\n\n        subgraph \"Monitoring &amp; Observability\"\n            METRICS[Prometheus]\n            LOGS[Grafana]\n            TRACE[Jaeger]\n        end\n    end</code></pre>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#api-server-rust","title":"API Server (Rust)","text":"<p>The API server is the heart of BlackLake, built with Rust for performance and safety:</p> <ul> <li>Framework: Axum for HTTP handling</li> <li>Authentication: JWT/OIDC integration</li> <li>Database: PostgreSQL with connection pooling</li> <li>Caching: Redis for session and query caching</li> <li>Search: Apache Solr integration</li> <li>Storage: S3-compatible object storage</li> </ul> <p>Key Features: - RESTful API with OpenAPI documentation - Rate limiting and DDoS protection - Request/response logging and metrics - Health checks and monitoring - Multi-tenant data isolation</p>"},{"location":"architecture/#database-layer","title":"Database Layer","text":""},{"location":"architecture/#postgresql-primary-database","title":"PostgreSQL (Primary Database)","text":"<ul> <li>Purpose: Primary data store for metadata, users, permissions</li> <li>Features: JSONB support for flexible metadata, full-text search</li> <li>Scaling: Read replicas for query distribution</li> <li>Backup: Automated backups with point-in-time recovery</li> </ul>"},{"location":"architecture/#redis-cache-sessions","title":"Redis (Cache &amp; Sessions)","text":"<ul> <li>Purpose: Session storage, query caching, job queues</li> <li>Features: Pub/sub for real-time updates, TTL for cache expiration</li> <li>Scaling: Cluster mode for high availability</li> <li>Persistence: RDB snapshots and AOF for durability</li> </ul>"},{"location":"architecture/#apache-solr-search-engine","title":"Apache Solr (Search Engine)","text":"<ul> <li>Purpose: Full-text search, faceted search, suggestions</li> <li>Features: Distributed search, real-time indexing, spell checking</li> <li>Scaling: SolrCloud for horizontal scaling</li> <li>Schema: Managed schema with dynamic fields</li> </ul>"},{"location":"architecture/#storage-layer","title":"Storage Layer","text":""},{"location":"architecture/#s3-compatible-storage","title":"S3-Compatible Storage","text":"<ul> <li>Purpose: Object storage for files and artifacts</li> <li>Features: Versioning, lifecycle policies, server-side encryption</li> <li>Scaling: Multi-region replication</li> <li>Access: Signed URLs for secure access</li> </ul>"},{"location":"architecture/#minio-development","title":"MinIO (Development)","text":"<ul> <li>Purpose: S3-compatible storage for development</li> <li>Features: Local development, testing, CI/CD</li> <li>Configuration: Single-node or distributed mode</li> </ul>"},{"location":"architecture/#job-processing","title":"Job Processing","text":""},{"location":"architecture/#apalis-framework","title":"Apalis Framework","text":"<ul> <li>Purpose: Background job processing</li> <li>Features: Job queues, retry logic, dead letter queues</li> <li>Scaling: Multiple workers with load balancing</li> <li>Monitoring: Job metrics and failure tracking</li> </ul>"},{"location":"architecture/#job-types","title":"Job Types","text":"<ul> <li>Metadata Extraction: Analyze files and extract metadata</li> <li>RDF Generation: Create semantic representations</li> <li>Export Jobs: Generate exports and reports</li> <li>Cleanup Jobs: Retention policy enforcement</li> <li>Webhook Delivery: Send notifications to external systems</li> </ul>"},{"location":"architecture/#data-flow","title":"Data Flow","text":""},{"location":"architecture/#file-upload-flow","title":"File Upload Flow","text":"<pre><code>sequenceDiagram\n    participant U as User\n    participant UI as Web UI\n    participant API as API Server\n    participant AUTH as Auth Service\n    participant DB as Database\n    participant S3 as S3 Storage\n    participant JOB as Job Processor\n    participant SOLR as Solr Index\n\n    U-&gt;&gt;UI: Upload file\n    UI-&gt;&gt;API: POST /api/v1/repos/{id}/files\n\n    API-&gt;&gt;AUTH: Validate JWT token\n    AUTH--&gt;&gt;API: Token valid\n\n    API-&gt;&gt;DB: Check repository permissions\n    DB--&gt;&gt;API: Permission granted\n\n    API-&gt;&gt;S3: Upload file to storage\n    S3--&gt;&gt;API: File uploaded successfully\n\n    API-&gt;&gt;DB: Create entry record\n    DB--&gt;&gt;API: Entry created\n\n    API-&gt;&gt;JOB: Queue metadata extraction job\n    JOB--&gt;&gt;API: Job queued\n\n    API--&gt;&gt;UI: Upload successful\n    UI--&gt;&gt;U: File uploaded\n\n    Note over JOB,SOLR: Background processing\n    JOB-&gt;&gt;S3: Download file for processing\n    S3--&gt;&gt;JOB: File content\n\n    JOB-&gt;&gt;JOB: Extract metadata\n    JOB-&gt;&gt;JOB: Generate checksum\n    JOB-&gt;&gt;JOB: Create RDF representation\n\n    JOB-&gt;&gt;DB: Update entry with metadata\n    DB--&gt;&gt;JOB: Entry updated\n\n    JOB-&gt;&gt;SOLR: Index document\n    SOLR--&gt;&gt;JOB: Document indexed\n\n    JOB-&gt;&gt;JOB: Mark job as completed</code></pre>"},{"location":"architecture/#search-flow","title":"Search Flow","text":"<pre><code>sequenceDiagram\n    participant U as User\n    participant UI as Web UI\n    participant API as API Server\n    participant AUTH as Auth Service\n    participant CACHE as Redis Cache\n    participant SOLR as Solr Index\n    participant DB as Database\n\n    U-&gt;&gt;UI: Enter search query\n    UI-&gt;&gt;API: GET /api/v1/search?q={query}\n\n    API-&gt;&gt;AUTH: Validate JWT token\n    AUTH--&gt;&gt;API: Token valid\n\n    API-&gt;&gt;CACHE: Check cache for query\n    alt Cache hit\n        CACHE--&gt;&gt;API: Cached results\n        API--&gt;&gt;UI: Return cached results\n    else Cache miss\n        API-&gt;&gt;SOLR: Search index\n        SOLR--&gt;&gt;API: Search results\n\n        API-&gt;&gt;DB: Get additional metadata\n        DB--&gt;&gt;API: Metadata retrieved\n\n        API-&gt;&gt;API: Combine and rank results\n\n        API-&gt;&gt;CACHE: Store results in cache\n        CACHE--&gt;&gt;API: Results cached\n\n        API--&gt;&gt;UI: Return search results\n    end\n\n    UI--&gt;&gt;U: Display results</code></pre>"},{"location":"architecture/#security-architecture","title":"Security Architecture","text":""},{"location":"architecture/#authentication-authorization","title":"Authentication &amp; Authorization","text":""},{"location":"architecture/#oidcjwt-integration","title":"OIDC/JWT Integration","text":"<ul> <li>Provider: External OIDC provider (Auth0, Okta, etc.)</li> <li>Tokens: JWT tokens with role-based claims</li> <li>Refresh: Automatic token refresh</li> <li>Validation: Token signature and expiration validation</li> </ul>"},{"location":"architecture/#multi-tenant-isolation","title":"Multi-tenant Isolation","text":"<ul> <li>Data Isolation: Repository-level access control</li> <li>Network Isolation: VPC and security groups</li> <li>Resource Quotas: Per-tenant resource limits</li> <li>Audit Logging: Complete audit trail</li> </ul>"},{"location":"architecture/#api-security","title":"API Security","text":"<ul> <li>Rate Limiting: Per-user and per-IP limits</li> <li>CORS: Configurable cross-origin policies</li> <li>CSRF Protection: Double-submit token pattern</li> <li>Input Validation: Comprehensive input sanitization</li> </ul>"},{"location":"architecture/#data-protection","title":"Data Protection","text":""},{"location":"architecture/#encryption","title":"Encryption","text":"<ul> <li>At Rest: AES-256 encryption for stored data</li> <li>In Transit: TLS 1.3 for all communications</li> <li>Key Management: AWS KMS or HashiCorp Vault</li> <li>Database: Transparent data encryption (TDE)</li> </ul>"},{"location":"architecture/#access-controls","title":"Access Controls","text":"<ul> <li>RBAC: Role-based access control</li> <li>ABAC: Attribute-based access control</li> <li>Fine-grained: Repository and file-level permissions</li> <li>Temporary: Time-limited access tokens</li> </ul>"},{"location":"architecture/#scalability-performance","title":"Scalability &amp; Performance","text":""},{"location":"architecture/#horizontal-scaling","title":"Horizontal Scaling","text":""},{"location":"architecture/#api-servers","title":"API Servers","text":"<ul> <li>Load Balancing: NGINX or cloud load balancer</li> <li>Stateless: No server-side session state</li> <li>Auto-scaling: Based on CPU and memory metrics</li> <li>Health Checks: Automated health monitoring</li> </ul>"},{"location":"architecture/#database-scaling","title":"Database Scaling","text":"<ul> <li>Read Replicas: Distribute read queries</li> <li>Connection Pooling: Efficient connection management</li> <li>Query Optimization: Indexing and query analysis</li> <li>Partitioning: Table partitioning for large datasets</li> </ul>"},{"location":"architecture/#storage-scaling","title":"Storage Scaling","text":"<ul> <li>S3: Virtually unlimited storage</li> <li>CDN: CloudFront for global distribution</li> <li>Caching: Multi-level caching strategy</li> <li>Compression: Automatic compression for large files</li> </ul>"},{"location":"architecture/#performance-optimization","title":"Performance Optimization","text":""},{"location":"architecture/#caching-strategy","title":"Caching Strategy","text":"<ul> <li>Application Cache: In-memory caching for frequently accessed data</li> <li>Database Cache: Query result caching</li> <li>CDN Cache: Static asset caching</li> <li>Search Cache: Search result caching</li> </ul>"},{"location":"architecture/#query-optimization","title":"Query Optimization","text":"<ul> <li>Indexing: Strategic database indexing</li> <li>Query Analysis: Slow query identification</li> <li>Connection Pooling: Efficient connection reuse</li> <li>Batch Operations: Bulk operations for efficiency</li> </ul>"},{"location":"architecture/#monitoring-observability","title":"Monitoring &amp; Observability","text":""},{"location":"architecture/#metrics-collection","title":"Metrics Collection","text":""},{"location":"architecture/#application-metrics","title":"Application Metrics","text":"<ul> <li>Request Rate: API requests per second</li> <li>Response Time: P50, P95, P99 latencies</li> <li>Error Rate: 4xx and 5xx error rates</li> <li>Business Metrics: User activity, data volume</li> </ul>"},{"location":"architecture/#infrastructure-metrics","title":"Infrastructure Metrics","text":"<ul> <li>CPU Usage: Server CPU utilization</li> <li>Memory Usage: RAM and swap usage</li> <li>Disk I/O: Storage performance</li> <li>Network I/O: Network throughput</li> </ul>"},{"location":"architecture/#database-metrics","title":"Database Metrics","text":"<ul> <li>Connection Pool: Active and idle connections</li> <li>Query Performance: Slow query identification</li> <li>Lock Contention: Database lock analysis</li> <li>Replication Lag: Read replica lag monitoring</li> </ul>"},{"location":"architecture/#logging","title":"Logging","text":""},{"location":"architecture/#structured-logging","title":"Structured Logging","text":"<ul> <li>Format: JSON structured logs</li> <li>Levels: DEBUG, INFO, WARN, ERROR</li> <li>Context: Request ID, user ID, correlation ID</li> <li>Sampling: Configurable log sampling</li> </ul>"},{"location":"architecture/#log-aggregation","title":"Log Aggregation","text":"<ul> <li>Collection: Fluentd or Fluent Bit</li> <li>Storage: Elasticsearch or cloud logging</li> <li>Search: Kibana or cloud search</li> <li>Retention: Configurable log retention</li> </ul>"},{"location":"architecture/#tracing","title":"Tracing","text":""},{"location":"architecture/#distributed-tracing","title":"Distributed Tracing","text":"<ul> <li>Framework: OpenTelemetry or Jaeger</li> <li>Instrumentation: Automatic and manual instrumentation</li> <li>Context Propagation: Request context across services</li> <li>Performance Analysis: End-to-end request tracing</li> </ul>"},{"location":"architecture/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"architecture/#container-orchestration","title":"Container Orchestration","text":""},{"location":"architecture/#kubernetes","title":"Kubernetes","text":"<ul> <li>Pods: Container orchestration</li> <li>Services: Service discovery and load balancing</li> <li>Ingress: External traffic routing</li> <li>ConfigMaps: Configuration management</li> <li>Secrets: Secret management</li> </ul>"},{"location":"architecture/#helm-charts","title":"Helm Charts","text":"<ul> <li>Templates: Kubernetes resource templates</li> <li>Values: Environment-specific configuration</li> <li>Dependencies: Chart dependencies</li> <li>Upgrades: Rolling updates and rollbacks</li> </ul>"},{"location":"architecture/#cicd-pipeline","title":"CI/CD Pipeline","text":""},{"location":"architecture/#github-actions","title":"GitHub Actions","text":"<ul> <li>Build: Docker image building</li> <li>Test: Automated testing</li> <li>Security: Vulnerability scanning</li> <li>Deploy: Automated deployment</li> </ul>"},{"location":"architecture/#deployment-strategies","title":"Deployment Strategies","text":"<ul> <li>Blue-Green: Zero-downtime deployments</li> <li>Rolling: Gradual rollout</li> <li>Canary: Traffic splitting for testing</li> <li>Rollback: Quick rollback capabilities</li> </ul>"},{"location":"architecture/#disaster-recovery","title":"Disaster Recovery","text":""},{"location":"architecture/#backup-strategy","title":"Backup Strategy","text":""},{"location":"architecture/#database-backups","title":"Database Backups","text":"<ul> <li>Frequency: Daily full backups</li> <li>Incremental: Hourly incremental backups</li> <li>Retention: 30-day retention policy</li> <li>Testing: Regular restore testing</li> </ul>"},{"location":"architecture/#storage-backups","title":"Storage Backups","text":"<ul> <li>Replication: Cross-region replication</li> <li>Versioning: Object versioning</li> <li>Lifecycle: Automated lifecycle policies</li> <li>Encryption: Backup encryption</li> </ul>"},{"location":"architecture/#high-availability","title":"High Availability","text":""},{"location":"architecture/#multi-az-deployment","title":"Multi-AZ Deployment","text":"<ul> <li>Availability Zones: Multi-AZ deployment</li> <li>Load Balancing: Cross-AZ load balancing</li> <li>Failover: Automatic failover</li> <li>Monitoring: Health check monitoring</li> </ul>"},{"location":"architecture/#active-standby","title":"Active-Standby","text":"<ul> <li>Primary: Active database</li> <li>Standby: Read-only replica</li> <li>Promotion: Manual promotion procedures</li> <li>Monitoring: Replication lag monitoring</li> </ul>"},{"location":"architecture/#development-workflow","title":"Development Workflow","text":""},{"location":"architecture/#local-development","title":"Local Development","text":""},{"location":"architecture/#docker-compose","title":"Docker Compose","text":"<ul> <li>Services: All services in one stack</li> <li>Networking: Service discovery</li> <li>Volumes: Persistent data storage</li> <li>Environment: Development configuration</li> </ul>"},{"location":"architecture/#hot-reloading","title":"Hot Reloading","text":"<ul> <li>API Server: Cargo watch for Rust</li> <li>Web UI: Vite hot module replacement</li> <li>Database: Schema migrations</li> <li>Testing: Automated testing</li> </ul>"},{"location":"architecture/#testing-strategy","title":"Testing Strategy","text":""},{"location":"architecture/#unit-tests","title":"Unit Tests","text":"<ul> <li>Coverage: 80%+ code coverage</li> <li>Isolation: Mock external dependencies</li> <li>Performance: Benchmark testing</li> <li>Security: Security testing</li> </ul>"},{"location":"architecture/#integration-tests","title":"Integration Tests","text":"<ul> <li>API Testing: End-to-end API testing</li> <li>Database Testing: Database integration testing</li> <li>Storage Testing: S3 integration testing</li> <li>Search Testing: Solr integration testing</li> </ul>"},{"location":"architecture/#load-testing","title":"Load Testing","text":"<ul> <li>k6 Scripts: Performance testing</li> <li>Metrics: Performance metrics collection</li> <li>Thresholds: Performance thresholds</li> <li>Reporting: Performance reports</li> </ul> <p>This architecture provides a solid foundation for a scalable, secure, and maintainable data platform. The modular design allows for independent scaling of components and easy addition of new features.</p>"},{"location":"cli/","title":"BlackLake CLI Documentation","text":"<p>The BlackLake CLI provides a command-line interface for managing data artifacts, repositories, and performing various operations.</p>"},{"location":"cli/#installation","title":"Installation","text":"<p>The CLI is included in the Docker Compose setup and can be used directly:</p> <pre><code># Start CLI service\ndocker-compose up cli\n\n# Or run interactively\ndocker-compose run --rm cli\n</code></pre>"},{"location":"cli/#basic-commands","title":"Basic Commands","text":""},{"location":"cli/#repository-management","title":"Repository Management","text":"<pre><code># List repositories\nblacklake-cli repos list\n\n# Get repository information\nblacklake-cli repos get &lt;repo-name&gt;\n\n# Create a new repository\nblacklake-cli repos create --name \"my-repo\" --description \"My ML Repository\"\n</code></pre>"},{"location":"cli/#file-operations","title":"File Operations","text":"<pre><code># Upload a file\nblacklake-cli put --file /data/myfile.txt --repo my-repo\n\n# Download a file\nblacklake-cli get --repo my-repo --path data/myfile.txt\n\n# List files in repository\nblacklake-cli ls --repo my-repo\n</code></pre>"},{"location":"cli/#search-operations","title":"Search Operations","text":"<pre><code># Search for files\nblacklake-cli search --query \"documentation\"\n\n# Search by metadata\nblacklake-cli search --metadata \"tags:ml,ai\"\n\n# Search by file type\nblacklake-cli search --type onnx\n</code></pre>"},{"location":"cli/#advanced-usage","title":"Advanced Usage","text":""},{"location":"cli/#configuration","title":"Configuration","text":"<p>The CLI can be configured using environment variables or a configuration file:</p> <pre><code># Set API endpoint\nexport BLACKLAKE_API_URL=http://localhost:8080\n\n# Set authentication token\nexport BLACKLAKE_TOKEN=your-jwt-token\n\n# Set default repository\nexport BLACKLAKE_DEFAULT_REPO=my-repo\n</code></pre>"},{"location":"cli/#batch-operations","title":"Batch Operations","text":"<pre><code># Upload multiple files\nblacklake-cli put --file /data/*.txt --repo my-repo\n\n# Search with filters\nblacklake-cli search --query \"model\" --type onnx --limit 10\n</code></pre>"},{"location":"cli/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli/#common-issues","title":"Common Issues","text":"<ol> <li>Authentication errors: Ensure you have a valid JWT token</li> <li>Connection errors: Check that the API service is running</li> <li>File not found: Verify the file path and repository name</li> </ol>"},{"location":"cli/#debug-mode","title":"Debug Mode","text":"<pre><code># Enable debug logging\nexport RUST_LOG=debug\nblacklake-cli --help\n</code></pre>"},{"location":"cli/#additional-resources","title":"Additional Resources","text":"<ul> <li>API Documentation</li> <li>Getting Started Guide</li> <li>Local Testing Guide</li> </ul>"},{"location":"documentation/","title":"Blacklake","text":"<p>A Rust-based, S3-backed, Git-style data artifact service for managing machine learning models and datasets with version control, content addressing, and metadata search capabilities.</p>"},{"location":"documentation/#features","title":"Features","text":"<ul> <li>Git-style Version Control: Commit, branch, and tag your data artifacts</li> <li>Content-Addressed Storage: SHA256-based deduplication with S3 backend</li> <li>Metadata Search: JSON Schema validation and PostgreSQL JSONB search</li> <li>Model Format Support: ONNX and PyTorch metadata extraction</li> <li>RESTful API: HTTP API with JWT/OIDC authentication</li> <li>Developer CLI: Command-line interface for common operations</li> <li>Docker Compose: Complete development environment with Postgres, MinIO, and Keycloak</li> <li>Multi-Arch Images: AMD64 and ARM64 support with Docker Buildx</li> <li>Production Ready: Comprehensive monitoring, security, and operations tooling</li> <li>Performance Testing: K6-based load, stress, and spike testing</li> </ul>"},{"location":"documentation/#architecture","title":"Architecture","text":"<p>Blacklake consists of six main crates:</p> <ul> <li><code>api</code>: Axum HTTP server with REST endpoints</li> <li><code>core</code>: Domain types, schemas, and business logic</li> <li><code>index</code>: PostgreSQL database access layer</li> <li><code>storage</code>: S3-compatible storage with presigned URLs</li> <li><code>modelx</code>: ONNX/PyTorch metadata sniffers</li> <li><code>cli</code>: Developer command-line interface</li> </ul>"},{"location":"documentation/#documentation","title":"\ud83d\udcda Documentation","text":"<p>Comprehensive documentation is available in the documentation directory:</p> <ul> <li>Project Status - Current project status and completion summary</li> <li>Fast Setup - Quick setup guide with build optimization</li> <li>Verification - Comprehensive verification of all systems</li> <li>Implementation Summary - Week-by-week implementation details</li> <li>Deployment - Production deployment guide</li> <li>Operations - Operations runbooks and procedures</li> </ul>"},{"location":"documentation/#quick-start","title":"Quick Start","text":""},{"location":"documentation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Rust 1.70+</li> <li>Docker and Docker Compose</li> <li><code>just</code> command runner (optional, for dev commands)</li> </ul>"},{"location":"documentation/#local-initialization","title":"Local Initialization","text":"<p>The <code>blacklake init</code> command helps you initialize directories and files as BlackLake artifacts with comprehensive metadata templates.</p>"},{"location":"documentation/#basic-initialization","title":"Basic Initialization","text":"<pre><code># Initialize a new BlackLake repository\nblacklake init\n\n# Initialize with a specific name\nblacklake init --name \"my-ml-project\"\n\n# Initialize with custom metadata\nblacklake init --name \"my-project\" --description \"Machine Learning Pipeline\"\n</code></pre>"},{"location":"documentation/#advanced-initialization","title":"Advanced Initialization","text":"<pre><code># Initialize with comprehensive metadata\nblacklake init \\\n  --name \"advanced-ml-project\" \\\n  --description \"Advanced Machine Learning Pipeline with MLOps\" \\\n  --author \"Data Science Team\" \\\n  --license \"MIT\" \\\n  --tags \"ml,ai,pipeline,production\" \\\n  --version \"1.0.0\"\n</code></pre>"},{"location":"documentation/#initialization-options","title":"Initialization Options","text":"<p>The <code>blacklake init</code> command supports various options:</p> <pre><code>blacklake init [OPTIONS]\n\nOptions:\n  -n, --name &lt;NAME&gt;              Repository name\n  -d, --description &lt;DESCRIPTION&gt; Repository description\n  -a, --author &lt;AUTHOR&gt;          Repository author\n  -l, --license &lt;LICENSE&gt;        Repository license\n  -t, --tags &lt;TAGS&gt;              Comma-separated tags\n  -v, --version &lt;VERSION&gt;        Repository version\n  -f, --force                    Overwrite existing repository\n  -h, --help                     Print help\n</code></pre>"},{"location":"documentation/#generated-structure","title":"Generated Structure","text":"<p>After initialization, you'll have:</p> <pre><code>my-project/\n\u251c\u2500\u2500 .blacklake/\n\u2502   \u251c\u2500\u2500 config.toml          # Repository configuration\n\u2502   \u251c\u2500\u2500 metadata.json       # Repository metadata\n\u2502   \u2514\u2500\u2500 .gitignore          # BlackLake-specific gitignore\n\u251c\u2500\u2500 data/                    # Your data files\n\u251c\u2500\u2500 models/                  # ML models\n\u251c\u2500\u2500 datasets/               # Training datasets\n\u2514\u2500\u2500 README.md               # Project documentation\n</code></pre>"},{"location":"documentation/#configuration-file","title":"Configuration File","text":"<p>The <code>.blacklake/config.toml</code> file contains:</p> <pre><code>[repository]\nname = \"my-project\"\ndescription = \"Machine Learning Pipeline\"\nauthor = \"Data Science Team\"\nlicense = \"MIT\"\nversion = \"1.0.0\"\ntags = [\"ml\", \"ai\", \"pipeline\"]\n\n[storage]\nbackend = \"s3\"\nbucket = \"blacklake\"\nregion = \"us-east-1\"\n\n[search]\nbackend = \"postgres\"\nindex_metadata = true\nindex_content = true\n\n[auth]\nprovider = \"oidc\"\nissuer = \"https://keycloak.example.com/realms/blacklake\"\n</code></pre>"},{"location":"documentation/#metadata-template","title":"Metadata Template","text":"<p>The <code>metadata.json</code> file provides a comprehensive template:</p> <pre><code>{\n  \"repository\": {\n    \"name\": \"my-project\",\n    \"description\": \"Machine Learning Pipeline\",\n    \"author\": \"Data Science Team\",\n    \"license\": \"MIT\",\n    \"version\": \"1.0.0\",\n    \"tags\": [\"ml\", \"ai\", \"pipeline\"],\n    \"created_at\": \"2024-01-15T10:00:00Z\",\n    \"updated_at\": \"2024-01-15T10:00:00Z\"\n  },\n  \"data\": {\n    \"format\": \"mixed\",\n    \"size\": \"0B\",\n    \"files\": 0,\n    \"directories\": 0\n  },\n  \"models\": {\n    \"count\": 0,\n    \"formats\": [],\n    \"total_size\": \"0B\"\n  },\n  \"datasets\": {\n    \"count\": 0,\n    \"formats\": [],\n    \"total_size\": \"0B\"\n  },\n  \"dependencies\": {\n    \"python\": [],\n    \"r\": [],\n    \"julia\": [],\n    \"other\": []\n  },\n  \"environment\": {\n    \"os\": \"linux\",\n    \"python_version\": \"3.11\",\n    \"r_version\": \"4.3\",\n    \"julia_version\": \"1.9\"\n  },\n  \"workflow\": {\n    \"stages\": [],\n    \"pipeline\": [],\n    \"artifacts\": []\n  },\n  \"compliance\": {\n    \"data_classification\": \"internal\",\n    \"retention_policy\": \"7y\",\n    \"access_control\": \"team\",\n    \"audit_logging\": true\n  }\n}\n</code></pre>"},{"location":"documentation/#development-setup","title":"Development Setup","text":""},{"location":"documentation/#1-clone-and-build","title":"1. Clone and Build","text":"<pre><code>git clone https://github.com/your-org/blacklake.git\ncd blacklake\n\n# Build all crates\ncargo build --workspace\n\n# Run tests\ncargo test --workspace\n</code></pre>"},{"location":"documentation/#2-start-services","title":"2. Start Services","text":"<pre><code># Start all services\ndocker-compose up -d\n\n# Or use just (if installed)\njust up-dev\n</code></pre>"},{"location":"documentation/#3-initialize-repository","title":"3. Initialize Repository","text":"<pre><code># Initialize a new repository\nblacklake init --name \"my-ml-project\"\n\n# Add some data\necho \"Hello, BlackLake!\" &gt; data/hello.txt\nblacklake add data/hello.txt\n\n# Commit the changes\nblacklake commit -m \"Initial commit\"\n</code></pre>"},{"location":"documentation/#api-usage","title":"API Usage","text":""},{"location":"documentation/#authentication","title":"Authentication","text":"<pre><code># Get JWT token from Keycloak\ncurl -X POST http://localhost:8081/realms/blacklake/protocol/openid-connect/token \\\n  -H \"Content-Type: application/x-www-form-urlencoded\" \\\n  -d \"username=admin&amp;password=admin&amp;grant_type=password&amp;client_id=blacklake\"\n</code></pre>"},{"location":"documentation/#repository-operations","title":"Repository Operations","text":"<pre><code># Create repository\ncurl -X POST http://localhost:8080/api/v1/repos \\\n  -H \"Authorization: Bearer $JWT_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"my-repo\", \"description\": \"My ML Repository\"}'\n\n# List repositories\ncurl -X GET http://localhost:8080/api/v1/repos \\\n  -H \"Authorization: Bearer $JWT_TOKEN\"\n\n# Upload file\ncurl -X POST http://localhost:8080/api/v1/repos/my-repo/upload \\\n  -H \"Authorization: Bearer $JWT_TOKEN\" \\\n  -F \"file=@data/model.onnx\"\n</code></pre>"},{"location":"documentation/#search-operations","title":"Search Operations","text":"<pre><code># Search files\ncurl -X GET \"http://localhost:8080/api/v1/search?q=model&amp;type=onnx\" \\\n  -H \"Authorization: Bearer $JWT_TOKEN\"\n\n# Search by metadata\ncurl -X POST http://localhost:8080/api/v1/search \\\n  -H \"Authorization: Bearer $JWT_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": {\"metadata.tags\": \"production\"}, \"limit\": 10}'\n</code></pre>"},{"location":"documentation/#production-deployment","title":"Production Deployment","text":""},{"location":"documentation/#docker-compose","title":"Docker Compose","text":"<pre><code># Production deployment\ndocker-compose -f docker-compose.prod.yml up -d\n\n# With custom configuration\nBLACKLAKE_DOMAIN=blacklake.example.com docker-compose -f docker-compose.prod.yml up -d\n</code></pre>"},{"location":"documentation/#kubernetes","title":"Kubernetes","text":"<pre><code># Deploy to Kubernetes\nkubectl apply -f k8s/\n\n# With Helm\nhelm install blacklake ./helm/blacklake -f helm/blacklake/values.yaml\n</code></pre>"},{"location":"documentation/#monitoring","title":"Monitoring","text":""},{"location":"documentation/#health-checks","title":"Health Checks","text":"<pre><code># API health\ncurl http://localhost:8080/health\n\n# Database health\ncurl http://localhost:8080/health/db\n\n# Storage health\ncurl http://localhost:8080/health/storage\n</code></pre>"},{"location":"documentation/#metrics","title":"Metrics","text":"<ul> <li>Prometheus: http://localhost:9090</li> <li>Grafana: http://localhost:3000 (admin/admin)</li> <li>Jaeger: http://localhost:16686</li> </ul>"},{"location":"documentation/#contributing","title":"Contributing","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Add tests</li> <li>Submit a pull request</li> </ol>"},{"location":"documentation/#license","title":"License","text":"<p>MIT License - see LICENSE file for details.</p>"},{"location":"documentation/#support","title":"Support","text":"<ul> <li>Documentation: Documentation Home</li> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> </ul>"},{"location":"getting-started/","title":"Getting Started with BlackLake","text":"<p>Welcome to BlackLake, a modern data platform designed for machine learning and data science workflows. This guide will help you get up and running quickly.</p>"},{"location":"getting-started/#what-is-blacklake","title":"What is BlackLake?","text":"<p>BlackLake is a comprehensive data platform that provides:</p> <ul> <li>Version Control for Data: Git-like versioning for datasets and models</li> <li>Metadata Management: Rich metadata extraction and indexing</li> <li>Search &amp; Discovery: Powerful search capabilities with faceted filtering</li> <li>Compliance &amp; Governance: Built-in data governance and compliance features</li> <li>Multi-tenant Architecture: Secure, isolated workspaces for teams</li> <li>API-First Design: RESTful APIs and SDKs for integration</li> </ul>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose</li> <li>Git</li> <li>Node.js 18+ (for UI development)</li> <li>Rust 1.70+ (for API development)</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":"<ol> <li> <p>Clone the repository <pre><code>git clone https://github.com/your-org/blacklake.git\ncd blacklake\n</code></pre></p> </li> <li> <p>Start the development environment <pre><code>docker-compose up -d\n</code></pre></p> </li> <li> <p>Verify installation <pre><code>curl http://localhost:8080/health\n</code></pre></p> </li> <li> <p>Access the web UI    Open http://localhost:3000 in your browser</p> </li> </ol>"},{"location":"getting-started/#first-steps","title":"First Steps","text":"<ol> <li> <p>Create your first repository <pre><code># Using the CLI\nblacklake repo create my-first-repo\n\n# Or via the web UI\n# Navigate to the \"Repositories\" section and click \"New Repository\"\n</code></pre></p> </li> <li> <p>Upload your first dataset <pre><code># Using the CLI\nblacklake upload my-first-repo ./data/my-dataset.csv\n\n# Or via the web UI\n# Navigate to your repository and click \"Upload Files\"\n</code></pre></p> </li> <li> <p>Search for your data <pre><code># Using the CLI\nblacklake search \"machine learning\"\n\n# Or via the web UI\n# Use the search bar at the top of the interface\n</code></pre></p> </li> </ol>"},{"location":"getting-started/#core-concepts","title":"Core Concepts","text":""},{"location":"getting-started/#repositories","title":"Repositories","text":"<p>Repositories are the primary organizational unit in BlackLake. They contain:</p> <ul> <li>Files: Your datasets, models, and other artifacts</li> <li>Metadata: Automatically extracted information about your files</li> <li>History: Complete version history of all changes</li> <li>Permissions: Access control for team members</li> </ul>"},{"location":"getting-started/#entries","title":"Entries","text":"<p>Entries represent individual files within a repository. Each entry includes:</p> <ul> <li>Content: The actual file data</li> <li>Metadata: Automatically extracted information</li> <li>Lineage: Relationships to other entries</li> <li>Versions: Complete change history</li> </ul>"},{"location":"getting-started/#metadata","title":"Metadata","text":"<p>BlackLake automatically extracts rich metadata from your files:</p> <ul> <li>Technical Metadata: File size, type, checksums</li> <li>Content Metadata: Schema, statistics, previews</li> <li>ML Metadata: Model architecture, training parameters</li> <li>Custom Metadata: User-defined tags and properties</li> </ul>"},{"location":"getting-started/#authentication","title":"Authentication","text":"<p>BlackLake supports multiple authentication methods:</p>"},{"location":"getting-started/#oidcjwt-recommended","title":"OIDC/JWT (Recommended)","text":"<pre><code># Set your JWT token\nexport BLACKLAKE_TOKEN=\"your-jwt-token\"\n\n# Or use the CLI login command\nblacklake auth login\n</code></pre>"},{"location":"getting-started/#api-keys","title":"API Keys","text":"<pre><code># Generate an API key via the web UI\n# Settings &gt; API Keys &gt; Generate New Key\n\n# Use the API key\nexport BLACKLAKE_API_KEY=\"your-api-key\"\n</code></pre>"},{"location":"getting-started/#cli-usage","title":"CLI Usage","text":"<p>The BlackLake CLI provides powerful command-line access to all features:</p>"},{"location":"getting-started/#repository-management","title":"Repository Management","text":"<pre><code># List repositories\nblacklake repo list\n\n# Create a new repository\nblacklake repo create my-repo --description \"My data repository\"\n\n# Get repository information\nblacklake repo info my-repo\n</code></pre>"},{"location":"getting-started/#file-operations","title":"File Operations","text":"<pre><code># Upload files\nblacklake upload my-repo ./data/file.csv\n\n# Download files\nblacklake download my-repo file.csv\n\n# List files in a repository\nblacklake ls my-repo\n</code></pre>"},{"location":"getting-started/#search","title":"Search","text":"<pre><code># Search across all repositories\nblacklake search \"machine learning\"\n\n# Search within a specific repository\nblacklake search \"model\" --repo my-repo\n\n# Advanced search with filters\nblacklake search \"data\" --type csv --size \"&gt;1MB\"\n</code></pre>"},{"location":"getting-started/#version-control","title":"Version Control","text":"<pre><code># Commit changes\nblacklake commit my-repo -m \"Add new dataset\"\n\n# View history\nblacklake log my-repo\n\n# Create branches\nblacklake branch my-repo create feature-branch\n</code></pre>"},{"location":"getting-started/#web-ui","title":"Web UI","text":"<p>The BlackLake web interface provides a modern, intuitive experience:</p>"},{"location":"getting-started/#dashboard","title":"Dashboard","text":"<ul> <li>Overview of your repositories</li> <li>Recent activity and changes</li> <li>Quick access to common tasks</li> </ul>"},{"location":"getting-started/#repository-view","title":"Repository View","text":"<ul> <li>File browser with preview capabilities</li> <li>Metadata display and editing</li> <li>Version history and branching</li> </ul>"},{"location":"getting-started/#search-interface","title":"Search Interface","text":"<ul> <li>Powerful search with faceted filtering</li> <li>Real-time suggestions and autocomplete</li> <li>Saved searches and bookmarks</li> </ul>"},{"location":"getting-started/#admin-panel","title":"Admin Panel","text":"<ul> <li>User and permission management</li> <li>System configuration</li> <li>Monitoring and analytics</li> </ul>"},{"location":"getting-started/#api-integration","title":"API Integration","text":"<p>BlackLake provides comprehensive REST APIs for integration:</p>"},{"location":"getting-started/#authentication_1","title":"Authentication","text":"<pre><code># Include your token in requests\ncurl -H \"Authorization: Bearer $BLACKLAKE_TOKEN\" \\\n     http://localhost:8080/api/v1/repos\n</code></pre>"},{"location":"getting-started/#basic-operations","title":"Basic Operations","text":"<pre><code>import requests\n\n# List repositories\nresponse = requests.get(\n    \"http://localhost:8080/api/v1/repos\",\n    headers={\"Authorization\": f\"Bearer {token}\"}\n)\nrepos = response.json()\n\n# Upload a file\nwith open(\"data.csv\", \"rb\") as f:\n    response = requests.post(\n        f\"http://localhost:8080/api/v1/repos/{repo_id}/files/data.csv\",\n        headers={\"Authorization\": f\"Bearer {token}\"},\n        data=f\n    )\n</code></pre>"},{"location":"getting-started/#python-sdk","title":"Python SDK","text":"<pre><code>from blacklake import BlackLakeClient\n\nclient = BlackLakeClient(token=\"your-token\")\n\n# Create a repository\nrepo = client.repos.create(\"my-repo\", description=\"My repository\")\n\n# Upload a file\nclient.files.upload(repo.id, \"data.csv\", \"path/to/data.csv\")\n\n# Search for files\nresults = client.search(\"machine learning\")\n</code></pre>"},{"location":"getting-started/#configuration","title":"Configuration","text":""},{"location":"getting-started/#environment-variables","title":"Environment Variables","text":"Variable Description Default <code>BLACKLAKE_API_URL</code> API server URL <code>http://localhost:8080</code> <code>BLACKLAKE_TOKEN</code> Authentication token - <code>BLACKLAKE_API_KEY</code> API key for authentication - <code>BLACKLAKE_DEBUG</code> Enable debug logging <code>false</code>"},{"location":"getting-started/#configuration-file","title":"Configuration File","text":"<p>Create a <code>~/.blacklake/config.toml</code> file:</p> <pre><code>[api]\nurl = \"http://localhost:8080\"\ntimeout = 30\n\n[auth]\ntoken = \"your-jwt-token\"\n# or\napi_key = \"your-api-key\"\n\n[logging]\nlevel = \"info\"\nformat = \"json\"\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have BlackLake running, explore these advanced features:</p> <ol> <li>Team Collaboration: Set up user permissions and team workspaces</li> <li>Advanced Search: Learn about faceted search and saved queries</li> <li>Metadata Management: Customize metadata extraction and schemas</li> <li>Compliance: Configure retention policies and audit logging</li> <li>Integration: Connect with your existing ML pipelines</li> </ol>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Browse the complete documentation</li> <li>Community: Join our community forum</li> <li>Support: Contact support for enterprise features</li> <li>GitHub: Report issues and contribute</li> </ul>"},{"location":"getting-started/#examples","title":"Examples","text":""},{"location":"getting-started/#data-science-workflow","title":"Data Science Workflow","text":"<pre><code># 1. Create a repository for your project\nblacklake repo create ml-project\n\n# 2. Upload your training data\nblacklake upload ml-project ./data/train.csv\nblacklake upload ml-project ./data/test.csv\n\n# 3. Upload your trained model\nblacklake upload ml-project ./models/model.pkl\n\n# 4. Commit your work\nblacklake commit ml-project -m \"Initial dataset and model\"\n\n# 5. Search for your work later\nblacklake search \"model\" --repo ml-project\n</code></pre>"},{"location":"getting-started/#team-collaboration","title":"Team Collaboration","text":"<pre><code># 1. Create a team repository\nblacklake repo create team-data --public\n\n# 2. Add team members (via web UI)\n# Navigate to repository settings and add users\n\n# 3. Set up branch protection\nblacklake branch team-data protect main\n\n# 4. Work on feature branches\nblacklake branch team-data create feature/new-dataset\nblacklake upload team-data ./new-data.csv\nblacklake commit team-data -m \"Add new dataset\"\n</code></pre> <p>This completes your introduction to BlackLake. You're now ready to start using the platform for your data science and machine learning workflows!</p>"},{"location":"local_testing/","title":"Local Testing Guide","text":"<p>This guide provides comprehensive instructions for setting up and testing the BlackLake project locally using Docker and optimized Rust builds.</p>"},{"location":"local_testing/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"local_testing/#one-command-setup","title":"One-Command Setup","text":"<pre><code># Complete setup in one command\njust setup-all &amp;&amp; just dev &amp;&amp; just migrate\n</code></pre>"},{"location":"local_testing/#prerequisites","title":"\ud83d\udccb Prerequisites","text":""},{"location":"local_testing/#required-tools","title":"Required Tools","text":"<pre><code># Install cargo-chef for optimal Docker builds\ncargo install cargo-chef\n\n# Install just (command runner)\nbrew install just\n\n# Install Docker and Docker Compose\n# Docker Desktop for Mac/Windows or Docker Engine for Linux\n</code></pre>"},{"location":"local_testing/#verify-installation","title":"Verify Installation","text":"<pre><code># Check tools are installed\ncargo --version\njust --version\ndocker --version\ndocker compose --version\n</code></pre>"},{"location":"local_testing/#building-services","title":"\ud83c\udfd7\ufe0f Building Services","text":""},{"location":"local_testing/#build-all-services-optimized","title":"Build All Services (Optimized)","text":"<pre><code># Build all services with cargo-chef optimization\ndocker buildx bake all\n\n# Or build individual services\ndocker buildx bake api-local\ndocker buildx bake cli-local\ndocker buildx bake ui-local\ndocker buildx bake gateway-local\n</code></pre>"},{"location":"local_testing/#build-for-specific-platform","title":"Build for Specific Platform","text":"<pre><code># Build for ARM64 (Apple Silicon)\ndocker buildx build --platform linux/arm64 -f Dockerfile.cli -t blacklake-cli:arm64 .\n\n# Build for AMD64 (Intel)\ndocker buildx build --platform linux/amd64 -f Dockerfile.cli -t blacklake-cli:amd64 .\n</code></pre>"},{"location":"local_testing/#starting-development-environment","title":"\ud83d\ude80 Starting Development Environment","text":""},{"location":"local_testing/#start-all-services","title":"Start All Services","text":"<pre><code># Start all services with migrations\njust dev\n\n# Or manually with docker-compose\ndocker compose --profile dev up -d --wait\n</code></pre>"},{"location":"local_testing/#run-database-migrations","title":"Run Database Migrations","text":"<pre><code># Run database migrations\njust migrate\n\n# Or manually\ndocker compose run --rm migrations\n</code></pre>"},{"location":"local_testing/#testing-cli-functionality","title":"\ud83e\uddea Testing CLI Functionality","text":""},{"location":"local_testing/#test-cli-commands","title":"Test CLI Commands","text":"<pre><code># Open CLI shell\njust cli-shell\n# Inside container: blacklake --help\n# Inside container: blacklake init --help\n# Inside container: blacklake put --help\n\n# Run specific CLI commands\njust cli-run \"init --help\"\njust cli-run \"put --help\"\n</code></pre>"},{"location":"local_testing/#test-cli-with-direct-build","title":"Test CLI with Direct Build","text":"<pre><code># Test with optimized build\n./test-cli-direct.sh\n\n# Test with standard build\n./test-cli-native.sh\n</code></pre>"},{"location":"local_testing/#accessing-services","title":"\ud83c\udf10 Accessing Services","text":""},{"location":"local_testing/#service-urls","title":"Service URLs","text":"<ul> <li>API: http://localhost:8080</li> <li>UI: http://localhost:3000</li> <li>MinIO: http://localhost:9001</li> <li>PostgreSQL: localhost:5432</li> </ul>"},{"location":"local_testing/#service-health-checks","title":"Service Health Checks","text":"<pre><code># Check API health\ncurl http://localhost:8080/health\n\n# Check UI\ncurl http://localhost:3000\n\n# Check MinIO\ncurl http://localhost:9001/minio/health/live\n</code></pre>"},{"location":"local_testing/#development-commands","title":"\ud83d\udd27 Development Commands","text":""},{"location":"local_testing/#view-logs","title":"View Logs","text":"<pre><code># View all logs\njust logs\n\n# View specific service logs\ndocker compose logs api\ndocker compose logs cli\ndocker compose logs db\n</code></pre>"},{"location":"local_testing/#stop-services","title":"Stop Services","text":"<pre><code># Stop all services\njust stop\n\n# Or manually\ndocker compose down\n</code></pre>"},{"location":"local_testing/#clean-up","title":"Clean Up","text":"<pre><code># Clean up containers and volumes\njust clean\n\n# Or manually\ndocker compose down -v\ndocker system prune -f\n</code></pre>"},{"location":"local_testing/#rebuild-services","title":"Rebuild Services","text":"<pre><code># Rebuild specific service\ndocker buildx bake api-local --no-cache\ndocker buildx bake cli-local --no-cache\n\n# Rebuild all services\ndocker buildx bake all --no-cache\n</code></pre>"},{"location":"local_testing/#testing-workflows","title":"\ud83e\uddea Testing Workflows","text":""},{"location":"local_testing/#test-complete-cli-workflow","title":"Test Complete CLI Workflow","text":"<pre><code># 1. Initialize a directory\njust cli-run \"init ./test-data --repo test-repo --ref main\"\n\n# 2. Upload files\njust cli-run \"put ./test-data --repo test-repo --ref main\"\n\n# 3. Search for files\njust cli-run \"search --query test\"\n</code></pre>"},{"location":"local_testing/#test-api-endpoints","title":"Test API Endpoints","text":"<pre><code># Test API endpoints\ncurl -X GET http://localhost:8080/v1/repos\ncurl -X GET http://localhost:8080/v1/repos/test-repo\ncurl -X GET http://localhost:8080/v1/repos/test-repo/tree/main\n</code></pre>"},{"location":"local_testing/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"local_testing/#common-issues","title":"Common Issues","text":""},{"location":"local_testing/#platform-mismatch-apple-silicon","title":"Platform Mismatch (Apple Silicon)","text":"<pre><code># Error: GLIBC version not found\n# Solution: Use ARM64 platform\ndocker buildx build --platform linux/arm64 -f Dockerfile.cli -t blacklake-cli:arm64 .\n</code></pre>"},{"location":"local_testing/#database-connection-issues","title":"Database Connection Issues","text":"<pre><code># Check database is running\ndocker compose ps db\n\n# Check database logs\ndocker compose logs db\n\n# Restart database\ndocker compose restart db\n</code></pre>"},{"location":"local_testing/#build-cache-issues","title":"Build Cache Issues","text":"<pre><code># Clear build cache\ndocker buildx prune -f\n\n# Rebuild without cache\ndocker buildx bake all --no-cache\n</code></pre>"},{"location":"local_testing/#debug-commands","title":"Debug Commands","text":"<pre><code># Check running containers\ndocker compose ps\n\n# Check container logs\ndocker compose logs -f api\n\n# Execute commands in running container\ndocker compose exec api bash\ndocker compose exec cli bash\n</code></pre>"},{"location":"local_testing/#performance-optimization","title":"\ud83d\udcca Performance Optimization","text":""},{"location":"local_testing/#build-optimization","title":"Build Optimization","text":"<ul> <li>cargo-chef: Dependency caching for faster builds</li> <li>Multi-stage builds: Smaller final images</li> <li>Layer caching: Reuse unchanged layers</li> <li>Multi-platform: ARM64 + AMD64 support</li> </ul>"},{"location":"local_testing/#runtime-optimization","title":"Runtime Optimization","text":"<ul> <li>Non-root user: Security best practices</li> <li>Minimal runtime: Debian slim base images</li> <li>Resource limits: Configured in docker-compose.yml</li> </ul>"},{"location":"local_testing/#development-workflow","title":"\ud83d\udd04 Development Workflow","text":""},{"location":"local_testing/#daily-development","title":"Daily Development","text":"<pre><code># Start development environment\njust dev\n\n# Make code changes\n# ... edit code ...\n\n# Rebuild affected services\ndocker buildx bake api-local\ndocker buildx bake cli-local\n\n# Test changes\njust cli-run \"init --help\"\n</code></pre>"},{"location":"local_testing/#testing-changes","title":"Testing Changes","text":"<pre><code># Run tests\njust test\n\n# Run specific test\njust cli-run \"init ./test-data --dry-run\"\n</code></pre>"},{"location":"local_testing/#additional-resources","title":"\ud83d\udcdd Additional Resources","text":""},{"location":"local_testing/#documentation","title":"Documentation","text":"<ul> <li>Implementation Summary</li> <li>Migration Setup</li> <li>API Documentation</li> </ul>"},{"location":"local_testing/#useful-commands","title":"Useful Commands","text":"<pre><code># Show all available just commands\njust --list\n\n# Show Docker Compose services\ndocker compose config --services\n\n# Show build targets\ndocker buildx bake --print\n</code></pre>"},{"location":"local_testing/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<ol> <li>Test CLI functionality with the provided test scripts</li> <li>Explore the API using the service URLs</li> <li>Run migrations to set up the database schema</li> <li>Test the complete workflow from init to upload</li> </ol> <p>For more detailed information, see the main documentation.md and IMPLEMENTATION_SUMMARY.md.</p>"},{"location":"ops/runbooks/backup-restore/","title":"Backup and Restore Runbook","text":""},{"location":"ops/runbooks/backup-restore/#overview","title":"Overview","text":"<p>This runbook provides step-by-step procedures for backing up and restoring BlackLake data.</p>"},{"location":"ops/runbooks/backup-restore/#backup-procedures","title":"Backup Procedures","text":""},{"location":"ops/runbooks/backup-restore/#database-backup","title":"Database Backup","text":"<pre><code># Create database backup\npg_dump -h localhost -U blacklake -d blacklake &gt; backup_$(date +%Y%m%d_%H%M%S).sql\n\n# Compress backup\ngzip backup_*.sql\n</code></pre>"},{"location":"ops/runbooks/backup-restore/#s3-storage-backup","title":"S3 Storage Backup","text":"<pre><code># Sync S3 bucket to backup location\naws s3 sync s3://blacklake-storage s3://blacklake-backup/$(date +%Y%m%d)/\n</code></pre>"},{"location":"ops/runbooks/backup-restore/#redis-backup","title":"Redis Backup","text":"<pre><code># Create Redis backup\nredis-cli --rdb /backup/redis_$(date +%Y%m%d_%H%M%S).rdb\n</code></pre>"},{"location":"ops/runbooks/backup-restore/#restore-procedures","title":"Restore Procedures","text":""},{"location":"ops/runbooks/backup-restore/#database-restore","title":"Database Restore","text":"<pre><code># Restore from backup\ngunzip -c backup_20240101_120000.sql.gz | psql -h localhost -U blacklake -d blacklake\n</code></pre>"},{"location":"ops/runbooks/backup-restore/#s3-storage-restore","title":"S3 Storage Restore","text":"<pre><code># Restore from backup\naws s3 sync s3://blacklake-backup/20240101/ s3://blacklake-storage/\n</code></pre>"},{"location":"ops/runbooks/backup-restore/#redis-restore","title":"Redis Restore","text":"<pre><code># Stop Redis\nsystemctl stop redis\n\n# Copy backup file\ncp /backup/redis_20240101_120000.rdb /var/lib/redis/dump.rdb\n\n# Start Redis\nsystemctl start redis\n</code></pre>"},{"location":"ops/runbooks/backup-restore/#verification","title":"Verification","text":"<ul> <li>Verify database connectivity</li> <li>Check S3 bucket contents</li> <li>Test Redis operations</li> <li>Run health checks</li> </ul>"},{"location":"ops/runbooks/disaster-recovery/","title":"Disaster Recovery Runbook","text":""},{"location":"ops/runbooks/disaster-recovery/#overview","title":"Overview","text":"<p>This runbook provides procedures for disaster recovery scenarios.</p>"},{"location":"ops/runbooks/disaster-recovery/#recovery-procedures","title":"Recovery Procedures","text":""},{"location":"ops/runbooks/disaster-recovery/#complete-system-recovery","title":"Complete System Recovery","text":"<ol> <li>Infrastructure Setup</li> <li>Provision new infrastructure</li> <li>Configure networking and security</li> <li> <p>Install required software</p> </li> <li> <p>Database Recovery</p> </li> <li>Restore from latest backup</li> <li>Verify data integrity</li> <li> <p>Update connection strings</p> </li> <li> <p>Storage Recovery</p> </li> <li>Restore S3 buckets</li> <li>Verify file integrity</li> <li> <p>Update storage configurations</p> </li> <li> <p>Application Recovery</p> </li> <li>Deploy application code</li> <li>Configure environment variables</li> <li> <p>Start services</p> </li> <li> <p>Verification</p> </li> <li>Run health checks</li> <li>Test critical functionality</li> <li>Monitor system performance</li> </ol>"},{"location":"ops/runbooks/disaster-recovery/#partial-recovery","title":"Partial Recovery","text":"<ul> <li>Identify affected components</li> <li>Restore specific services</li> <li>Verify functionality</li> <li>Update monitoring</li> </ul>"},{"location":"ops/runbooks/disaster-recovery/#recovery-time-objectives-rto","title":"Recovery Time Objectives (RTO)","text":"<ul> <li>Critical Systems: 4 hours</li> <li>Non-Critical Systems: 24 hours</li> <li>Full Recovery: 48 hours</li> </ul>"},{"location":"ops/runbooks/disaster-recovery/#recovery-point-objectives-rpo","title":"Recovery Point Objectives (RPO)","text":"<ul> <li>Database: 1 hour</li> <li>Storage: 4 hours</li> <li>Configuration: 24 hours</li> </ul>"},{"location":"ops/runbooks/performance-troubleshooting/","title":"Performance Troubleshooting Runbook","text":""},{"location":"ops/runbooks/performance-troubleshooting/#overview","title":"Overview","text":"<p>This runbook provides procedures for diagnosing and resolving performance issues.</p>"},{"location":"ops/runbooks/performance-troubleshooting/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"ops/runbooks/performance-troubleshooting/#key-metrics","title":"Key Metrics","text":"<ul> <li>Response Time: API endpoint response times</li> <li>Throughput: Requests per second</li> <li>Error Rate: Failed requests percentage</li> <li>Resource Usage: CPU, memory, disk, network</li> </ul>"},{"location":"ops/runbooks/performance-troubleshooting/#monitoring-tools","title":"Monitoring Tools","text":"<ul> <li>Prometheus metrics</li> <li>Grafana dashboards</li> <li>Application logs</li> <li>System monitoring</li> </ul>"},{"location":"ops/runbooks/performance-troubleshooting/#common-performance-issues","title":"Common Performance Issues","text":""},{"location":"ops/runbooks/performance-troubleshooting/#high-response-time","title":"High Response Time","text":"<ol> <li>Database Issues</li> <li>Slow queries</li> <li>Connection pool exhaustion</li> <li>Lock contention</li> <li> <p>Missing indexes</p> </li> <li> <p>Application Issues</p> </li> <li>Inefficient algorithms</li> <li>Memory leaks</li> <li>Blocking operations</li> <li> <p>Resource contention</p> </li> <li> <p>Infrastructure Issues</p> </li> <li>Network latency</li> <li>Disk I/O bottlenecks</li> <li>CPU saturation</li> <li>Memory pressure</li> </ol>"},{"location":"ops/runbooks/performance-troubleshooting/#troubleshooting-steps","title":"Troubleshooting Steps","text":"<ol> <li>Identify Bottlenecks</li> <li>Check system metrics</li> <li>Analyze application logs</li> <li>Review database performance</li> <li> <p>Monitor network traffic</p> </li> <li> <p>Root Cause Analysis</p> </li> <li>Profile application code</li> <li>Analyze query performance</li> <li>Check resource utilization</li> <li> <p>Review configuration</p> </li> <li> <p>Resolution</p> </li> <li>Optimize queries</li> <li>Tune application settings</li> <li>Scale resources</li> <li>Update configurations</li> </ol>"},{"location":"ops/runbooks/performance-troubleshooting/#performance-optimization","title":"Performance Optimization","text":""},{"location":"ops/runbooks/performance-troubleshooting/#database-optimization","title":"Database Optimization","text":"<ul> <li>Add missing indexes</li> <li>Optimize query plans</li> <li>Tune connection pools</li> <li>Update statistics</li> </ul>"},{"location":"ops/runbooks/performance-troubleshooting/#application-optimization","title":"Application Optimization","text":"<ul> <li>Profile code performance</li> <li>Optimize algorithms</li> <li>Reduce memory usage</li> <li>Implement caching</li> </ul>"},{"location":"ops/runbooks/performance-troubleshooting/#infrastructure-optimization","title":"Infrastructure Optimization","text":"<ul> <li>Scale resources</li> <li>Optimize network</li> <li>Tune system parameters</li> <li>Update configurations</li> </ul>"},{"location":"ops/runbooks/performance-troubleshooting/#performance-testing","title":"Performance Testing","text":"<ul> <li>Load testing</li> <li>Stress testing</li> <li>Capacity planning</li> <li>Benchmarking</li> </ul>"},{"location":"ops/runbooks/security-incident/","title":"Security Incident Response Runbook","text":""},{"location":"ops/runbooks/security-incident/#overview","title":"Overview","text":"<p>This runbook provides procedures for responding to security incidents.</p>"},{"location":"ops/runbooks/security-incident/#incident-classification","title":"Incident Classification","text":""},{"location":"ops/runbooks/security-incident/#critical-p1","title":"Critical (P1)","text":"<ul> <li>Data breach</li> <li>Unauthorized access</li> <li>System compromise</li> <li>Malware detection</li> </ul>"},{"location":"ops/runbooks/security-incident/#high-p2","title":"High (P2)","text":"<ul> <li>Suspicious activity</li> <li>Failed authentication attempts</li> <li>Unusual network traffic</li> <li>Policy violations</li> </ul>"},{"location":"ops/runbooks/security-incident/#medium-p3","title":"Medium (P3)","text":"<ul> <li>Security warnings</li> <li>Configuration issues</li> <li>Access anomalies</li> <li>Audit findings</li> </ul>"},{"location":"ops/runbooks/security-incident/#response-procedures","title":"Response Procedures","text":""},{"location":"ops/runbooks/security-incident/#immediate-response","title":"Immediate Response","text":"<ol> <li>Containment</li> <li>Isolate affected systems</li> <li>Preserve evidence</li> <li> <p>Document timeline</p> </li> <li> <p>Assessment</p> </li> <li>Determine scope of impact</li> <li>Identify affected data</li> <li> <p>Assess system integrity</p> </li> <li> <p>Communication</p> </li> <li>Notify stakeholders</li> <li>Update status page</li> <li>Escalate as needed</li> </ol>"},{"location":"ops/runbooks/security-incident/#investigation","title":"Investigation","text":"<ol> <li>Evidence Collection</li> <li>System logs</li> <li>Network traffic</li> <li>User activity</li> <li> <p>Configuration snapshots</p> </li> <li> <p>Analysis</p> </li> <li>Root cause analysis</li> <li>Impact assessment</li> <li> <p>Timeline reconstruction</p> </li> <li> <p>Documentation</p> </li> <li>Incident report</li> <li>Lessons learned</li> <li>Recommendations</li> </ol>"},{"location":"ops/runbooks/security-incident/#recovery","title":"Recovery","text":"<ol> <li>System Restoration</li> <li>Clean compromised systems</li> <li>Restore from clean backups</li> <li> <p>Verify integrity</p> </li> <li> <p>Security Hardening</p> </li> <li>Update security controls</li> <li>Patch vulnerabilities</li> <li> <p>Review access controls</p> </li> <li> <p>Monitoring</p> </li> <li>Enhanced monitoring</li> <li>Additional logging</li> <li>Regular reviews</li> </ol>"},{"location":"ops/runbooks/security-incident/#escalation-matrix","title":"Escalation Matrix","text":"<ul> <li>Level 1: Security Team</li> <li>Level 2: Engineering Management</li> <li>Level 3: Executive Team</li> <li>Level 4: Legal/Compliance</li> </ul>"}]}